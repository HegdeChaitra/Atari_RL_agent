{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/atari_game/logger.py:7: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/traitlets-4.3.2-py3.6.egg/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 112, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/asyncio/base_events.py\", line 1426, in _run_once\n",
      "    handle._run()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/asyncio/events.py\", line 127, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 102, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/ipython-6.2.1-py3.6.egg/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/ipython-6.2.1-py3.6.egg/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/ipython-6.2.1-py3.6.egg/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-13425d297397>\", line 32, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/matplotlib-2.1.0-py3.6-linux-x86_64.egg/matplotlib/pyplot.py\", line 69, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/matplotlib-2.1.0-py3.6-linux-x86_64.egg/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use(\"Agg\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import itertools\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import argparse\n",
    "from torch import optim\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "from torch.utils.data import Sampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import seaborn\n",
    "import datetime\n",
    "from logger import Logger\n",
    "\n",
    "from torch.optim import RMSprop\n",
    "import shutil\n",
    "from statistics import mean\n",
    "\n",
    "from gym_wrappers import MainGymWrapper\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, action_space):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.action_space = action_space\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=8, stride=(4,4))\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64,kernel_size=4,stride=(2,2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        o_size = 3136\n",
    "        self.linear1 = nn.Linear(in_features=o_size, out_features=512)\n",
    "        self.linear2 = nn.Linear(in_features=512, out_features=self.action_space)\n",
    "        \n",
    "    def forward(self, x, batch_size=None):\n",
    "        x = torch.from_numpy(x).float().to(device)\n",
    "#         print(x.size(), torch.Size([1, 4, 84, 84]))\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "\n",
    "        # out = (bs, nc, x, y)\n",
    "        if batch_size==None:\n",
    "            batch_size = out.size(0)\n",
    "#         print(out.size())\n",
    "        out_flat = out.view(batch_size, -1)\n",
    "        \n",
    "        out_flat = F.relu(self.linear1(out_flat))\n",
    "        \n",
    "        out_flat = self.linear2(out_flat)\n",
    "        return out_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = \"Breakout-v0\"\n",
    "game_mode = 'ddqn_training'\n",
    "render = False\n",
    "total_step_limit = 5000000\n",
    "total_run_limit = None\n",
    "clip = True\n",
    "\n",
    "FRAMES_IN_OBSERVATION = 4\n",
    "FRAME_SIZE = 84\n",
    "INPUT_SHAPE = (FRAMES_IN_OBSERVATION, FRAME_SIZE, FRAME_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Breakout-v0', 'Breakout-v0Deterministic-v4')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = game_name + \"Deterministic-v4\" \n",
    "game_name, env_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = MainGymWrapper.wrap(gym.make(game_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "MEMORY_SIZE = 900000\n",
    "BATCH_SIZE = 32\n",
    "TRAINING_FREQUENCY = 4\n",
    "TARGET_NETWORK_UPDATE_FREQUENCY = 40000\n",
    "MODEL_PERSISTENCE_UPDATE_FREQUENCY = 10000\n",
    "REPLAY_START_SIZE = 50000\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.1\n",
    "EXPLORATION_TEST = 0.02\n",
    "EXPLORATION_STEPS = 850000\n",
    "EXPLORATION_DECAY = (EXPLORATION_MAX-EXPLORATION_MIN)/EXPLORATION_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNTrainer:\n",
    "    def __init__(self, game_name, input_shape, action_space):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.game_name = game_name\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.game_mode = \"DDQN training\"\n",
    "        self.model_path = \"./output/neural_nets/\" + game_name + \"/ddqn/\" + self._get_date() + \"/model.h5\"\n",
    "        self.logger_path = \"./output/logs/\" + game_name + \"/ddqn/training/\" + self._get_date() + \"/\"\n",
    "        self.logger = Logger(self.game_name + \" \" + self.game_mode, self.logger_path)\n",
    "        \n",
    "        if os.path.exists(os.path.dirname(self.model_path)):\n",
    "            shutil.rmtree(os.path.dirname(self.model_path), ignore_errors=True)\n",
    "        os.makedirs(os.path.dirname(self.model_path))\n",
    "\n",
    "        self.ddqn = ConvolutionalNeuralNetwork(self.input_shape, action_space).to(device)\n",
    "        \n",
    "        self.criteria = nn.MSELoss()\n",
    "        self.optimizer = RMSprop(self.ddqn.parameters(), lr=0.00025, alpha = 0.95, eps=0.01)\n",
    "        \n",
    "        if os.path.isfile(self.model_path):\n",
    "            self.ddqn.load_state_dict(torch.load(self.model_path))\n",
    "        \n",
    "        self.ddqn_target = ConvolutionalNeuralNetwork(self.input_shape, action_space).to(device)\n",
    "#         .model\n",
    "        self._reset_target_network()\n",
    "        self.epsilon = EXPLORATION_MAX\n",
    "        self.memory = []\n",
    "    \n",
    "    def _save_model(self):\n",
    "        torch.save(self.ddqn.state_dict(),self.model_path)\n",
    "        \n",
    "    def _get_date(self):\n",
    "        return str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "        \n",
    "    def save_run(self, score, step, run):\n",
    "        self.logger.add_score(score)\n",
    "        self.logger.add_step(step)\n",
    "        self.logger.add_run(run)\n",
    "\n",
    "    def move(self, state):\n",
    "        if np.random.rand() < self.epsilon or len(self.memory) < REPLAY_START_SIZE:\n",
    "            return random.randrange(self.action_space)\n",
    "        \n",
    "        q_values = self.ddqn(np.expand_dims(np.asarray(state).astype(np.float64), axis=0), batch_size=1)\n",
    "        return np.argmax(q_values[0].detach().cpu().numpy())\n",
    "\n",
    "    def remember(self, current_state, action, reward, next_state, terminal):\n",
    "        self.memory.append({\"current_state\": current_state,\n",
    "                            \"action\": action,\n",
    "                            \"reward\": reward,\n",
    "                            \"next_state\": next_state,\n",
    "                            \"terminal\": terminal})\n",
    "        if len(self.memory) > MEMORY_SIZE:\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def step_update(self, total_step):\n",
    "        if len(self.memory) < REPLAY_START_SIZE:\n",
    "            return\n",
    "\n",
    "        if total_step % TRAINING_FREQUENCY == 0:\n",
    "            loss, accuracy, average_max_q = self._train()\n",
    "            self.logger.add_loss(loss.item())\n",
    "            self.logger.add_accuracy(accuracy)\n",
    "            self.logger.add_q(average_max_q)\n",
    "\n",
    "        self._update_epsilon()\n",
    "\n",
    "        if total_step % MODEL_PERSISTENCE_UPDATE_FREQUENCY == 0:\n",
    "            self._save_model()\n",
    "\n",
    "        if total_step % TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "            self._reset_target_network()\n",
    "            print('{{\"metric\": \"epsilon\", \"value\": {}}}'.format(self.epsilon))\n",
    "            print('{{\"metric\": \"total_step\", \"value\": {}}}'.format(total_step))\n",
    "\n",
    "    def _train(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        self.ddqn_target.eval()\n",
    "        batch = np.asarray(random.sample(self.memory, BATCH_SIZE))\n",
    "        if len(batch) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        current_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        terminals = []\n",
    "        \n",
    "        for entry in batch:\n",
    "            current_state = np.expand_dims(np.asarray(entry[\"current_state\"]).astype(np.float64), axis=0)\n",
    "            current_states.append(current_state)\n",
    "            next_state = np.expand_dims(np.asarray(entry[\"next_state\"]).astype(np.float64), axis=0)\n",
    "            \n",
    "            next_states.append(next_state)\n",
    "            rewards.append(entry['reward'])\n",
    "            actions.append(entry['action'])\n",
    "            terminals.append(entry['terminal'])\n",
    "            \n",
    "        non_final_mask = torch.tensor(terminals)==False\n",
    "        non_final_next_states = np.asarray(next_states).squeeze()\n",
    "        \n",
    "        state_batch = np.asarray(current_states).squeeze()\n",
    "        action_batch = np.asanyarray(actions).squeeze()\n",
    "        reward_batch = np.asarray(rewards).squeeze()\n",
    "        state_action_values = self.ddqn(state_batch).gather(1, torch.from_numpy(action_batch).unsqueeze(dim=1).to(device))\n",
    "    \n",
    "        next_state_values = np.zeros(BATCH_SIZE)\n",
    "        temp = np.max(self.ddqn_target(non_final_next_states).detach().cpu().numpy(),axis=1)\n",
    "        \n",
    "        \n",
    "        next_state_values[list(np.array(terminals)==False)] = temp[list(np.array(terminals)==False)]\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "        \n",
    "#         loss = self.criteria(state_action_values, torch.from_numpy(expected_state_action_values).unsqueeze(1))\n",
    "        loss = F.smooth_l1_loss(state_action_values, torch.from_numpy(expected_state_action_values).unsqueeze(1).to(device).float())\n",
    "        loss.backward()\n",
    "        \n",
    "        for param in self.ddqn.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "#         print(torch.from_numpy(action_batch).unsqueeze(dim=1).size(),torch.Size([32, 1]))\n",
    "#         print(state_action_values.size(),[32, 4])\n",
    "#         print(state_action_values.size(),torch.Size([32, 1]))\n",
    "#         print(non_final_next_states.squeeze().size(), torch.Size([32, 4, 84, 84]))\n",
    "        \n",
    "        accuracy = 0\n",
    "        return loss, accuracy, mean(expected_state_action_values)\n",
    "\n",
    "    def _update_epsilon(self):\n",
    "        self.epsilon -= EXPLORATION_DECAY\n",
    "        self.epsilon = max(EXPLORATION_MIN, self.epsilon)\n",
    "\n",
    "    def _reset_target_network(self):\n",
    "        self.ddqn_target.load_state_dict(self.ddqn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(game_model, env, render, total_step_limit, total_run_limit, clip):\n",
    "    run = 0\n",
    "    total_step = 0\n",
    "    while True:\n",
    "        if run%10==0:\n",
    "            print('-'*100)\n",
    "            print(\"RUN = \",run)\n",
    "        if total_run_limit is not None and run >= total_run_limit:\n",
    "            print(\"Reached total run limit of: \" + str(total_run_limit))\n",
    "            exit(0)\n",
    "\n",
    "        run += 1\n",
    "        current_state = env.reset()\n",
    "        step = 0\n",
    "        score = 0\n",
    "\n",
    "        while True:\n",
    "            if total_step >= total_step_limit:\n",
    "                print(\"Reached total step limit of: \" + str(total_step_limit))\n",
    "                exit(0)\n",
    "            total_step += 1\n",
    "            \n",
    "            step += 1\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            action = game_model.move(current_state)\n",
    "            next_state, reward, terminal, info = env.step(action)\n",
    "            if clip:\n",
    "                np.sign(reward)\n",
    "            score += reward\n",
    "            game_model.remember(current_state, action, reward, next_state, terminal)\n",
    "            current_state = next_state\n",
    "\n",
    "            game_model.step_update(total_step)\n",
    "\n",
    "            if terminal:\n",
    "                game_model.save_run(score, step, run)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'gym_wrappers.NoopResetEnv'> doesn't implement 'step' method, which is required for wrappers derived directly from Wrapper. Deprecated default implementation is used.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: (min: 0.0, avg: 1.2, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.2}\n",
      "step: (min: 164, avg: 231.2, max: 292\n",
      "{\"metric\": \"step\", \"value\": 231.2}\n",
      "{\"metric\": \"run\", \"value\": 10}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  10\n",
      "score: (min: 0.0, avg: 1.2, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.2}\n",
      "step: (min: 172, avg: 238.9, max: 313\n",
      "{\"metric\": \"step\", \"value\": 238.9}\n",
      "{\"metric\": \"run\", \"value\": 20}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  20\n",
      "score: (min: 0.0, avg: 1.5, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.5}\n",
      "step: (min: 156, avg: 248.6, max: 330\n",
      "{\"metric\": \"step\", \"value\": 248.6}\n",
      "{\"metric\": \"run\", \"value\": 30}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  30\n",
      "score: (min: 0.0, avg: 1.3, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 1.3}\n",
      "step: (min: 171, avg: 238.1, max: 278\n",
      "{\"metric\": \"step\", \"value\": 238.1}\n",
      "{\"metric\": \"run\", \"value\": 40}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  40\n",
      "score: (min: 0.0, avg: 1.3, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.3}\n",
      "step: (min: 159, avg: 243.1, max: 306\n",
      "{\"metric\": \"step\", \"value\": 243.1}\n",
      "{\"metric\": \"run\", \"value\": 50}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  50\n",
      "score: (min: 0.0, avg: 1.4, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 162, avg: 241.2, max: 338\n",
      "{\"metric\": \"step\", \"value\": 241.2}\n",
      "{\"metric\": \"run\", \"value\": 60}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  60\n",
      "score: (min: 0.0, avg: 1.3, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.3}\n",
      "step: (min: 176, avg: 241.7, max: 337\n",
      "{\"metric\": \"step\", \"value\": 241.7}\n",
      "{\"metric\": \"run\", \"value\": 70}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  70\n",
      "score: (min: 0.0, avg: 1.0, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 1.0}\n",
      "step: (min: 159, avg: 220.8, max: 293\n",
      "{\"metric\": \"step\", \"value\": 220.8}\n",
      "{\"metric\": \"run\", \"value\": 80}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  80\n",
      "score: (min: 0.0, avg: 1.4, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 161, avg: 249, max: 388\n",
      "{\"metric\": \"step\", \"value\": 249}\n",
      "{\"metric\": \"run\", \"value\": 90}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  90\n",
      "score: (min: 0.0, avg: 1.7, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.7}\n",
      "step: (min: 161, avg: 262, max: 406\n",
      "{\"metric\": \"step\", \"value\": 262}\n",
      "{\"metric\": \"run\", \"value\": 100}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  100\n",
      "score: (min: 0.0, avg: 0.6, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 0.6}\n",
      "step: (min: 142, avg: 202, max: 326\n",
      "{\"metric\": \"step\", \"value\": 202}\n",
      "{\"metric\": \"run\", \"value\": 110}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  110\n",
      "score: (min: 0.0, avg: 1.1, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 165, avg: 223.7, max: 271\n",
      "{\"metric\": \"step\", \"value\": 223.7}\n",
      "{\"metric\": \"run\", \"value\": 120}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  120\n",
      "score: (min: 0.0, avg: 1.1, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 181, avg: 233.3, max: 311\n",
      "{\"metric\": \"step\", \"value\": 233.3}\n",
      "{\"metric\": \"run\", \"value\": 130}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  130\n",
      "score: (min: 0.0, avg: 0.6, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 0.6}\n",
      "step: (min: 164, avg: 198.5, max: 259\n",
      "{\"metric\": \"step\", \"value\": 198.5}\n",
      "{\"metric\": \"run\", \"value\": 140}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  140\n",
      "score: (min: 0.0, avg: 1.3, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.3}\n",
      "step: (min: 162, avg: 239.4, max: 407\n",
      "{\"metric\": \"step\", \"value\": 239.4}\n",
      "{\"metric\": \"run\", \"value\": 150}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  150\n",
      "score: (min: 0.0, avg: 1.6, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.6}\n",
      "step: (min: 168, avg: 267.1, max: 443\n",
      "{\"metric\": \"step\", \"value\": 267.1}\n",
      "{\"metric\": \"run\", \"value\": 160}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  160\n",
      "score: (min: 0.0, avg: 2.1, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 2.1}\n",
      "step: (min: 162, avg: 290.3, max: 468\n",
      "{\"metric\": \"step\", \"value\": 290.3}\n",
      "{\"metric\": \"run\", \"value\": 170}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  170\n",
      "score: (min: 0.0, avg: 1.0, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.0}\n",
      "step: (min: 163, avg: 230.3, max: 340\n",
      "{\"metric\": \"step\", \"value\": 230.3}\n",
      "{\"metric\": \"run\", \"value\": 180}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  180\n",
      "score: (min: 0.0, avg: 1.2, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 1.2}\n",
      "step: (min: 160, avg: 235.9, max: 295\n",
      "{\"metric\": \"step\", \"value\": 235.9}\n",
      "{\"metric\": \"run\", \"value\": 190}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  190\n",
      "score: (min: 0.0, avg: 1.1, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 161, avg: 226.9, max: 382\n",
      "{\"metric\": \"step\", \"value\": 226.9}\n",
      "{\"metric\": \"run\", \"value\": 200}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  200\n",
      "score: (min: 0.0, avg: 1.2, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.2}\n",
      "step: (min: 177, avg: 242.3, max: 331\n",
      "{\"metric\": \"step\", \"value\": 242.3}\n",
      "{\"metric\": \"run\", \"value\": 210}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  210\n",
      "score: (min: 0.0, avg: 1.7, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 1.7}\n",
      "step: (min: 148, avg: 256, max: 453\n",
      "{\"metric\": \"step\", \"value\": 256}\n",
      "{\"metric\": \"run\", \"value\": 220}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  220\n",
      "loss: (min: 0.00031161424703896046, avg: 0.015988584403414278, max: 5\n",
      "{\"metric\": \"loss\", \"value\": 0.015988584403414278}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.8119096659310162, avg: 0.8789428992859274, max: 0.9747229366376996\n",
      "{\"metric\": \"q\", \"value\": 0.8789428992859274}\n",
      "score: (min: 0.0, avg: 0.8, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 0.8}\n",
      "step: (min: 164, avg: 212.1, max: 280\n",
      "{\"metric\": \"step\", \"value\": 212.1}\n",
      "{\"metric\": \"run\", \"value\": 230}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  230\n",
      "score: (min: 0.0, avg: 1.1, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 165, avg: 234.1, max: 348\n",
      "{\"metric\": \"step\", \"value\": 234.1}\n",
      "{\"metric\": \"run\", \"value\": 240}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  240\n",
      "loss: (min: 0.00013542320812121034, avg: 0.00517883380803687, max: 0.0489041768014431\n",
      "{\"metric\": \"loss\", \"value\": 0.00517883380803687}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.8055414100550115, avg: 0.8774733320269361, max: 0.968487090524286\n",
      "{\"metric\": \"q\", \"value\": 0.8774733320269361}\n",
      "score: (min: 0.0, avg: 1.1, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 167, avg: 234, max: 339\n",
      "{\"metric\": \"step\", \"value\": 234}\n",
      "{\"metric\": \"run\", \"value\": 250}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  250\n",
      "score: (min: 0.0, avg: 1.0, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.0}\n",
      "step: (min: 147, avg: 223, max: 354\n",
      "{\"metric\": \"step\", \"value\": 223}\n",
      "{\"metric\": \"run\", \"value\": 260}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  260\n",
      "loss: (min: 0.00018325421842746437, avg: 0.0049928771405102455, max: 0.05051176995038986\n",
      "{\"metric\": \"loss\", \"value\": 0.0049928771405102455}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.8052237409539521, avg: 0.8769676480262727, max: 0.9526173811033368\n",
      "{\"metric\": \"q\", \"value\": 0.8769676480262727}\n",
      "score: (min: 0.0, avg: 1.0, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 1.0}\n",
      "step: (min: 165, avg: 222.6, max: 266\n",
      "{\"metric\": \"step\", \"value\": 222.6}\n",
      "{\"metric\": \"run\", \"value\": 270}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  270\n",
      "loss: (min: 0.0001503029779996723, avg: 0.0049983161857089725, max: 0.04703950509428978\n",
      "{\"metric\": \"loss\", \"value\": 0.0049983161857089725}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.8095598771609366, avg: 0.8772307236149721, max: 0.9541644685342908\n",
      "{\"metric\": \"q\", \"value\": 0.8772307236149721}\n",
      "score: (min: 0.0, avg: 1.4, max: 6.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 167, avg: 245.8, max: 498\n",
      "{\"metric\": \"step\", \"value\": 245.8}\n",
      "{\"metric\": \"run\", \"value\": 280}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  280\n",
      "score: (min: 0.0, avg: 1.0, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 1.0}\n",
      "step: (min: 169, avg: 226.4, max: 285\n",
      "{\"metric\": \"step\", \"value\": 226.4}\n",
      "{\"metric\": \"run\", \"value\": 290}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  290\n",
      "loss: (min: 0.0001321136805927381, avg: 0.004583645988226635, max: 0.051920659840106964\n",
      "{\"metric\": \"loss\", \"value\": 0.004583645988226635}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.8071912240795791, avg: 0.8760142332018725, max: 0.9775907176360488\n",
      "{\"metric\": \"q\", \"value\": 0.8760142332018725}\n",
      "score: (min: 0.0, avg: 1.8, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.8}\n",
      "step: (min: 148, avg: 263.7, max: 377\n",
      "{\"metric\": \"step\", \"value\": 263.7}\n",
      "{\"metric\": \"run\", \"value\": 300}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  300\n",
      "loss: (min: 0.00012139313912484795, avg: 0.005104625236112042, max: 0.057873450219631195\n",
      "{\"metric\": \"loss\", \"value\": 0.005104625236112042}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.8123166740499437, avg: 0.8770606293331087, max: 0.9774340571835637\n",
      "{\"metric\": \"q\", \"value\": 0.8770606293331087}\n",
      "score: (min: 1.0, avg: 2.4, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 2.4}\n",
      "step: (min: 210, avg: 302.6, max: 409\n",
      "{\"metric\": \"step\", \"value\": 302.6}\n",
      "{\"metric\": \"run\", \"value\": 310}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  310\n",
      "score: (min: 0.0, avg: 1.2, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.2}\n",
      "step: (min: 174, avg: 235.6, max: 331\n",
      "{\"metric\": \"step\", \"value\": 235.6}\n",
      "{\"metric\": \"run\", \"value\": 320}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  320\n",
      "loss: (min: 0.0001407862437190488, avg: 0.0050890081984980495, max: 0.04166029393672943\n",
      "{\"metric\": \"loss\", \"value\": 0.0050890081984980495}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.804069248791784, avg: 0.8772126366000809, max: 0.9791803760826587\n",
      "{\"metric\": \"q\", \"value\": 0.8772126366000809}\n",
      "score: (min: 0.0, avg: 1.3, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.3}\n",
      "step: (min: 161, avg: 235.3, max: 323\n",
      "{\"metric\": \"step\", \"value\": 235.3}\n",
      "{\"metric\": \"run\", \"value\": 330}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  330\n",
      "{\"metric\": \"epsilon\", \"value\": 0.9682342352941838}\n",
      "{\"metric\": \"total_step\", \"value\": 80000}\n",
      "score: (min: 0.0, avg: 1.3, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 1.3}\n",
      "step: (min: 164, avg: 234.7, max: 430\n",
      "{\"metric\": \"step\", \"value\": 234.7}\n",
      "{\"metric\": \"run\", \"value\": 340}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  340\n",
      "loss: (min: 6.22077495791018e-05, avg: 0.0044100442137787466, max: 0.041634395718574524\n",
      "{\"metric\": \"loss\", \"value\": 0.0044100442137787466}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.8256703548878431, avg: 0.8818293654084578, max: 0.9556617158278823\n",
      "{\"metric\": \"q\", \"value\": 0.8818293654084578}\n",
      "score: (min: 0.0, avg: 1.1, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 164, avg: 227.1, max: 287\n",
      "{\"metric\": \"step\", \"value\": 227.1}\n",
      "{\"metric\": \"run\", \"value\": 350}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  350\n",
      "score: (min: 0.0, avg: 0.9, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 0.9}\n",
      "step: (min: 170, avg: 228.8, max: 356\n",
      "{\"metric\": \"step\", \"value\": 228.8}\n",
      "{\"metric\": \"run\", \"value\": 360}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  360\n",
      "loss: (min: 6.864210445201024e-05, avg: 0.00477347393992386, max: 0.04447619989514351\n",
      "{\"metric\": \"loss\", \"value\": 0.00477347393992386}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.8174561735987663, avg: 0.8834689939782582, max: 0.9609049266763031\n",
      "{\"metric\": \"q\", \"value\": 0.8834689939782582}\n",
      "score: (min: 0.0, avg: 1.1, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 165, avg: 233.3, max: 304\n",
      "{\"metric\": \"step\", \"value\": 233.3}\n",
      "{\"metric\": \"run\", \"value\": 370}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  370\n",
      "loss: (min: 6.637100887019187e-05, avg: 0.004180093609676987, max: 0.04599782079458237\n",
      "{\"metric\": \"loss\", \"value\": 0.004180093609676987}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.8158534429036081, avg: 0.8823170056866482, max: 0.9659448270872235\n",
      "{\"metric\": \"q\", \"value\": 0.8823170056866482}\n",
      "score: (min: 0.0, avg: 1.5, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.5}\n",
      "step: (min: 178, avg: 250.1, max: 373\n",
      "{\"metric\": \"step\", \"value\": 250.1}\n",
      "{\"metric\": \"run\", \"value\": 380}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  380\n",
      "score: (min: 0.0, avg: 2.0, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 2.0}\n",
      "step: (min: 181, avg: 276.9, max: 349\n",
      "{\"metric\": \"step\", \"value\": 276.9}\n",
      "{\"metric\": \"run\", \"value\": 390}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  390\n",
      "loss: (min: 5.656742723658681e-05, avg: 0.004430672595415672, max: 0.052819978445768356\n",
      "{\"metric\": \"loss\", \"value\": 0.004430672595415672}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.7989158066175878, avg: 0.8832578610296175, max: 0.9532003381103277\n",
      "{\"metric\": \"q\", \"value\": 0.8832578610296175}\n",
      "score: (min: 0.0, avg: 1.4, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 165, avg: 244.5, max: 302\n",
      "{\"metric\": \"step\", \"value\": 244.5}\n",
      "{\"metric\": \"run\", \"value\": 400}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  400\n",
      "loss: (min: 4.253099541529082e-05, avg: 0.004151383933822217, max: 0.04030122607946396\n",
      "{\"metric\": \"loss\", \"value\": 0.004151383933822217}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n"
     ]
    }
   ],
   "source": [
    "game_model = DDQNTrainer(game_name, INPUT_SHAPE, env.action_space.n)\n",
    "main_loop(game_model, env, render, total_step_limit, total_run_limit, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_FREQUENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNTrainer:\n",
    "    def __init__(self, game_name, input_shape, action_space):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.game_name = game_name\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.game_mode = \"DDQN training\"\n",
    "        self.model_path = \"./output/neural_nets/\" + game_name + \"/ddqn/\" + self._get_date() + \"/model.h5\"\n",
    "        self.logger_path = \"./output/logs/\" + game_name + \"/ddqn/training/\" + self._get_date() + \"/\"\n",
    "        self.logger = Logger(self.game_name + \" \" + self.game_mode, self.logger_path)\n",
    "        \n",
    "        if os.path.exists(os.path.dirname(self.model_path)):\n",
    "            shutil.rmtree(os.path.dirname(self.model_path), ignore_errors=True)\n",
    "        os.makedirs(os.path.dirname(self.model_path))\n",
    "\n",
    "        self.ddqn = ConvolutionalNeuralNetwork(self.input_shape, action_space).to(device)\n",
    "        \n",
    "        self.criteria = nn.MSELoss()\n",
    "        self.optimizer = RMSprop(self.ddqn.parameters(), lr=0.00025, alpha = 0.95, eps=0.01)\n",
    "        \n",
    "        if os.path.isfile(self.model_path):\n",
    "            self.ddqn.load_state_dict(torch.load(self.model_path))\n",
    "        \n",
    "        self.ddqn_target = ConvolutionalNeuralNetwork(self.input_shape, action_space).to(device)\n",
    "#         .model\n",
    "        self._reset_target_network()\n",
    "        self.epsilon = EXPLORATION_MAX\n",
    "        self.memory = []\n",
    "    \n",
    "    def _save_model(self):\n",
    "        torch.save(self.ddqn.state_dict(),self.model_path)\n",
    "#         self.ddqn.save_weights(self.model_path)\n",
    "        \n",
    "    def _get_date(self):\n",
    "        return str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "        \n",
    "    def save_run(self, score, step, run):\n",
    "        self.logger.add_score(score)\n",
    "        self.logger.add_step(step)\n",
    "        self.logger.add_run(run)\n",
    "\n",
    "    def move(self, state):\n",
    "        if np.random.rand() < self.epsilon or len(self.memory) < REPLAY_START_SIZE:\n",
    "            return random.randrange(self.action_space)\n",
    "        \n",
    "        q_values = self.ddqn(np.expand_dims(np.asarray(state).astype(np.float64), axis=0), batch_size=1)\n",
    "        return np.argmax(q_values[0].detach().cpu().numpy())\n",
    "\n",
    "    def remember(self, current_state, action, reward, next_state, terminal):\n",
    "        self.memory.append({\"current_state\": current_state,\n",
    "                            \"action\": action,\n",
    "                            \"reward\": reward,\n",
    "                            \"next_state\": next_state,\n",
    "                            \"terminal\": terminal})\n",
    "        if len(self.memory) > MEMORY_SIZE:\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def step_update(self, total_step):\n",
    "        if len(self.memory) < REPLAY_START_SIZE:\n",
    "            return\n",
    "\n",
    "        if total_step % TRAINING_FREQUENCY == 0:\n",
    "            loss, accuracy, average_max_q = self._train()\n",
    "            self.logger.add_loss(loss.item())\n",
    "            self.logger.add_accuracy(accuracy)\n",
    "            self.logger.add_q(average_max_q)\n",
    "\n",
    "        self._update_epsilon()\n",
    "\n",
    "        if total_step % MODEL_PERSISTENCE_UPDATE_FREQUENCY == 0:\n",
    "            self._save_model()\n",
    "\n",
    "        if total_step % TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "            self._reset_target_network()\n",
    "            print('{{\"metric\": \"epsilon\", \"value\": {}}}'.format(self.epsilon))\n",
    "            print('{{\"metric\": \"total_step\", \"value\": {}}}'.format(total_step))\n",
    "\n",
    "    def _train(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        batch = np.asarray(random.sample(self.memory, BATCH_SIZE))\n",
    "        if len(batch) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        current_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        terminals = []\n",
    "        \n",
    "        q_values = []\n",
    "        max_q_values = []\n",
    "        \n",
    "#         print(batch)\n",
    "        for entry in batch:\n",
    "            current_state = np.expand_dims(np.asarray(entry[\"current_state\"]).astype(np.float64), axis=0)\n",
    "            current_states.append(current_state)\n",
    "            next_state = np.expand_dims(np.asarray(entry[\"next_state\"]).astype(np.float64), axis=0)\n",
    "            \n",
    "            next_states.append(next_state)\n",
    "            rewards.append(entry['reward'])\n",
    "            actions.append(entry['action'])\n",
    "            terminals.append(entry['terminal'])\n",
    "            \n",
    "        non_final_mask = torch.tensor(terminals)==False\n",
    "        non_final_next_states = np.asarray(next_states).squeeze()\n",
    "        \n",
    "        state_batch = np.asarray(current_states).squeeze()\n",
    "        action_batch = np.asanyarray(actions).squeeze()\n",
    "        reward_batch = np.asarray(rewards).squeeze()\n",
    "#         print(torch.from_numpy(action_batch).unsqueeze(dim=1).size(),torch.Size([32, 1]))\n",
    "        state_action_values = self.ddqn(state_batch).gather(1, torch.from_numpy(action_batch).unsqueeze(dim=1).to(device))\n",
    "    \n",
    "#         next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        next_state_values = np.zeros(BATCH_SIZE)\n",
    "        next_state_values[non_final_mask] = np.max(self.ddqn_target(non_final_next_states).detach().cpu().numpy())\n",
    "#         .max(1)[0].detach().cpu()    \n",
    "    \n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "        \n",
    "        loss = self.criteria(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "#         print(state_action_values.size(),[32, 4])\n",
    "#         print(state_action_values.size(),torch.Size([32, 1]))\n",
    "#         print(non_final_next_states.squeeze().size(), torch.Size([32, 4, 84, 84]))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "#             next_state_prediction = self.ddqn_target(next_state).detach().cpu().numpy().ravel()\n",
    "# #             print(next_state_prediction.shape, (4,))\n",
    "            \n",
    "#             next_q_value = np.max(next_state_prediction)\n",
    "#             q = list(self.ddqn(current_state)[0].detach().cpu().numpy())\n",
    "# #             print(len(q), 4)\n",
    "#             if entry[\"terminal\"]:\n",
    "#                 q[entry[\"action\"]] = entry[\"reward\"]\n",
    "#             else:\n",
    "#                 q[entry[\"action\"]] = entry[\"reward\"] + GAMMA * next_q_value\n",
    "#             q_values.append(q)\n",
    "#             max_q_values.append(np.max(q))\n",
    "        \n",
    "#         model_out = self.ddqn(np.asarray(current_states).squeeze())\n",
    "#         loss = self.criteria(model_out, torch.from_numpy(np.array(q_values)).float().to(device))\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "    \n",
    "\n",
    "        accuracy = 0\n",
    "        return loss, accuracy, mean(max_q_values)\n",
    "\n",
    "    def _update_epsilon(self):\n",
    "        self.epsilon -= EXPLORATION_DECAY\n",
    "        self.epsilon = max(EXPLORATION_MIN, self.epsilon)\n",
    "\n",
    "    def _reset_target_network(self):\n",
    "        self.ddqn_target.load_state_dict(self.ddqn.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
