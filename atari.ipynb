{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/atari_game/logger.py:7: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/traitlets-4.3.2-py3.6.egg/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 112, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/asyncio/base_events.py\", line 1426, in _run_once\n",
      "    handle._run()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/asyncio/events.py\", line 127, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 102, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/ipython-6.2.1-py3.6.egg/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/ipython-6.2.1-py3.6.egg/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/ipython-6.2.1-py3.6.egg/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-13425d297397>\", line 32, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/matplotlib-2.1.0-py3.6-linux-x86_64.egg/matplotlib/pyplot.py\", line 69, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/matplotlib-2.1.0-py3.6-linux-x86_64.egg/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use(\"Agg\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import itertools\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import argparse\n",
    "from torch import optim\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "from torch.utils.data import Sampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import seaborn\n",
    "import datetime\n",
    "from logger import Logger\n",
    "\n",
    "from torch.optim import RMSprop\n",
    "import shutil\n",
    "from statistics import mean\n",
    "\n",
    "from gym_wrappers import MainGymWrapper\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, action_space):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.action_space = action_space\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=8, stride=(4,4))\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64,kernel_size=4,stride=(2,2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        o_size = 3136\n",
    "        self.linear1 = nn.Linear(in_features=o_size, out_features=512)\n",
    "        self.linear2 = nn.Linear(in_features=512, out_features=self.action_space)\n",
    "        \n",
    "    def forward(self, x, batch_size=None):\n",
    "        x = torch.from_numpy(x).float().to(device)\n",
    "#         print(x.size(), torch.Size([1, 4, 84, 84]))\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "\n",
    "        # out = (bs, nc, x, y)\n",
    "        if batch_size==None:\n",
    "            batch_size = out.size(0)\n",
    "#         print(out.size())\n",
    "        out_flat = out.view(batch_size, -1)\n",
    "        \n",
    "        out_flat = F.relu(self.linear1(out_flat))\n",
    "        \n",
    "        out_flat = self.linear2(out_flat)\n",
    "        return out_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = \"Breakout-v0\"\n",
    "game_mode = 'ddqn_training'\n",
    "render = False\n",
    "total_step_limit = 5000000\n",
    "total_run_limit = None\n",
    "clip = True\n",
    "\n",
    "FRAMES_IN_OBSERVATION = 4\n",
    "FRAME_SIZE = 84\n",
    "INPUT_SHAPE = (FRAMES_IN_OBSERVATION, FRAME_SIZE, FRAME_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Breakout-v0', 'Breakout-v0Deterministic-v4')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = game_name + \"Deterministic-v4\" \n",
    "game_name, env_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = MainGymWrapper.wrap(gym.make(game_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "MEMORY_SIZE = 900000\n",
    "BATCH_SIZE = 32\n",
    "TRAINING_FREQUENCY = 4\n",
    "TARGET_NETWORK_UPDATE_FREQUENCY = 40000\n",
    "MODEL_PERSISTENCE_UPDATE_FREQUENCY = 10000\n",
    "REPLAY_START_SIZE = 50000\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.1\n",
    "EXPLORATION_TEST = 0.02\n",
    "EXPLORATION_STEPS = 850000\n",
    "EXPLORATION_DECAY = (EXPLORATION_MAX-EXPLORATION_MIN)/EXPLORATION_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNTrainer:\n",
    "    def __init__(self, game_name, input_shape, action_space):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.game_name = game_name\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.game_mode = \"DDQN training\"\n",
    "        self.model_path = \"./output/neural_nets/\" + game_name + \"/ddqn/\" + self._get_date() + \"/model.h5\"\n",
    "        self.logger_path = \"./output/logs/\" + game_name + \"/ddqn/training/\" + self._get_date() + \"/\"\n",
    "        self.logger = Logger(self.game_name + \" \" + self.game_mode, self.logger_path)\n",
    "        \n",
    "        if os.path.exists(os.path.dirname(self.model_path)):\n",
    "            shutil.rmtree(os.path.dirname(self.model_path), ignore_errors=True)\n",
    "        os.makedirs(os.path.dirname(self.model_path))\n",
    "\n",
    "        self.ddqn = ConvolutionalNeuralNetwork(self.input_shape, action_space).to(device)\n",
    "        \n",
    "        self.criteria = nn.MSELoss()\n",
    "        self.optimizer = RMSprop(self.ddqn.parameters(), lr=0.00025, alpha = 0.95, eps=0.01)\n",
    "        \n",
    "        if os.path.isfile(self.model_path):\n",
    "            self.ddqn.load_state_dict(torch.load(self.model_path))\n",
    "        \n",
    "        self.ddqn_target = ConvolutionalNeuralNetwork(self.input_shape, action_space).to(device)\n",
    "#         .model\n",
    "        self._reset_target_network()\n",
    "        self.epsilon = EXPLORATION_MAX\n",
    "        self.memory = []\n",
    "    \n",
    "    def _save_model(self):\n",
    "        torch.save(self.ddqn.state_dict(),self.model_path)\n",
    "        \n",
    "    def _get_date(self):\n",
    "        return str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "        \n",
    "    def save_run(self, score, step, run):\n",
    "        self.logger.add_score(score)\n",
    "        self.logger.add_step(step)\n",
    "        self.logger.add_run(run)\n",
    "\n",
    "    def move(self, state):\n",
    "        if np.random.rand() < self.epsilon or len(self.memory) < REPLAY_START_SIZE:\n",
    "            return random.randrange(self.action_space)\n",
    "        \n",
    "        q_values = self.ddqn(np.expand_dims(np.asarray(state).astype(np.float64), axis=0), batch_size=1)\n",
    "        return np.argmax(q_values[0].detach().cpu().numpy())\n",
    "\n",
    "    def remember(self, current_state, action, reward, next_state, terminal):\n",
    "        self.memory.append({\"current_state\": current_state,\n",
    "                            \"action\": action,\n",
    "                            \"reward\": reward,\n",
    "                            \"next_state\": next_state,\n",
    "                            \"terminal\": terminal})\n",
    "        if len(self.memory) > MEMORY_SIZE:\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def step_update(self, total_step):\n",
    "        if len(self.memory) < REPLAY_START_SIZE:\n",
    "            return\n",
    "\n",
    "        if total_step % TRAINING_FREQUENCY == 0:\n",
    "            loss, accuracy, average_max_q = self._train()\n",
    "            self.logger.add_loss(loss.item())\n",
    "            self.logger.add_accuracy(accuracy)\n",
    "            self.logger.add_q(average_max_q)\n",
    "\n",
    "        self._update_epsilon()\n",
    "\n",
    "        if total_step % MODEL_PERSISTENCE_UPDATE_FREQUENCY == 0:\n",
    "            self._save_model()\n",
    "\n",
    "        if total_step % TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "            self._reset_target_network()\n",
    "            print('{{\"metric\": \"epsilon\", \"value\": {}}}'.format(self.epsilon))\n",
    "            print('{{\"metric\": \"total_step\", \"value\": {}}}'.format(total_step))\n",
    "\n",
    "    def _train(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        self.ddqn_target.eval()\n",
    "        batch = np.asarray(random.sample(self.memory, BATCH_SIZE))\n",
    "        if len(batch) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        current_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        terminals = []\n",
    "        \n",
    "        for entry in batch:\n",
    "            current_state = np.expand_dims(np.asarray(entry[\"current_state\"]).astype(np.float64), axis=0)\n",
    "            current_states.append(current_state)\n",
    "            next_state = np.expand_dims(np.asarray(entry[\"next_state\"]).astype(np.float64), axis=0)\n",
    "            \n",
    "            next_states.append(next_state)\n",
    "            rewards.append(entry['reward'])\n",
    "            actions.append(entry['action'])\n",
    "            terminals.append(entry['terminal'])\n",
    "            \n",
    "        non_final_mask = torch.tensor(terminals)==False\n",
    "        non_final_next_states = np.asarray(next_states).squeeze()\n",
    "        \n",
    "        state_batch = np.asarray(current_states).squeeze()\n",
    "        action_batch = np.asanyarray(actions).squeeze()\n",
    "        reward_batch = np.asarray(rewards).squeeze()\n",
    "        state_action_values = self.ddqn(state_batch).gather(1, torch.from_numpy(action_batch).unsqueeze(dim=1).to(device))\n",
    "    \n",
    "        next_state_values = np.zeros(BATCH_SIZE)\n",
    "        next_state_values[non_final_mask] = np.max(self.ddqn_target(non_final_next_states).detach().cpu().numpy())\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "        \n",
    "#         loss = self.criteria(state_action_values, torch.from_numpy(expected_state_action_values).unsqueeze(1))\n",
    "        loss = F.smooth_l1_loss(state_action_values, torch.from_numpy(expected_state_action_values).unsqueeze(1).to(device).float())\n",
    "        loss.backward()\n",
    "        \n",
    "        for param in self.ddqn.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "#         print(torch.from_numpy(action_batch).unsqueeze(dim=1).size(),torch.Size([32, 1]))\n",
    "#         print(state_action_values.size(),[32, 4])\n",
    "#         print(state_action_values.size(),torch.Size([32, 1]))\n",
    "#         print(non_final_next_states.squeeze().size(), torch.Size([32, 4, 84, 84]))\n",
    "        \n",
    "        accuracy = 0\n",
    "        return loss, accuracy, mean(expected_state_action_values)\n",
    "\n",
    "    def _update_epsilon(self):\n",
    "        self.epsilon -= EXPLORATION_DECAY\n",
    "        self.epsilon = max(EXPLORATION_MIN, self.epsilon)\n",
    "\n",
    "    def _reset_target_network(self):\n",
    "        self.ddqn_target.load_state_dict(self.ddqn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(game_model, env, render, total_step_limit, total_run_limit, clip):\n",
    "    run = 0\n",
    "    total_step = 0\n",
    "    while True:\n",
    "        if total_run_limit is not None and run >= total_run_limit:\n",
    "            print(\"Reached total run limit of: \" + str(total_run_limit))\n",
    "            exit(0)\n",
    "\n",
    "        run += 1\n",
    "        current_state = env.reset()\n",
    "        step = 0\n",
    "        score = 0\n",
    "        while True:\n",
    "            if total_step >= total_step_limit:\n",
    "                print(\"Reached total step limit of: \" + str(total_step_limit))\n",
    "                exit(0)\n",
    "            total_step += 1\n",
    "            step += 1\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            action = game_model.move(current_state)\n",
    "            next_state, reward, terminal, info = env.step(action)\n",
    "            if clip:\n",
    "                np.sign(reward)\n",
    "            score += reward\n",
    "            game_model.remember(current_state, action, reward, next_state, terminal)\n",
    "            current_state = next_state\n",
    "\n",
    "            game_model.step_update(total_step)\n",
    "\n",
    "            if terminal:\n",
    "                game_model.save_run(score, step, run)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: (min: 0.0, avg: 1.6, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.6}\n",
      "step: (min: 165, avg: 261.2, max: 355\n",
      "{\"metric\": \"step\", \"value\": 261.2}\n",
      "{\"metric\": \"run\", \"value\": 10}\n",
      "score: (min: 0.0, avg: 1.2, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.2}\n",
      "step: (min: 163, avg: 235.9, max: 362\n",
      "{\"metric\": \"step\", \"value\": 235.9}\n",
      "{\"metric\": \"run\", \"value\": 20}\n",
      "score: (min: 0.0, avg: 1.3, max: 6.0\n",
      "{\"metric\": \"score\", \"value\": 1.3}\n",
      "step: (min: 163, avg: 240.6, max: 469\n",
      "{\"metric\": \"step\", \"value\": 240.6}\n",
      "{\"metric\": \"run\", \"value\": 30}\n",
      "score: (min: 0.0, avg: 1.4, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 168, avg: 249.7, max: 324\n",
      "{\"metric\": \"step\", \"value\": 249.7}\n",
      "{\"metric\": \"run\", \"value\": 40}\n",
      "score: (min: 0.0, avg: 1.1, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 165, avg: 230.8, max: 424\n",
      "{\"metric\": \"step\", \"value\": 230.8}\n",
      "{\"metric\": \"run\", \"value\": 50}\n",
      "score: (min: 0.0, avg: 0.7, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 0.7}\n",
      "step: (min: 161, avg: 209.2, max: 270\n",
      "{\"metric\": \"step\", \"value\": 209.2}\n",
      "{\"metric\": \"run\", \"value\": 60}\n",
      "score: (min: 0.0, avg: 1.5, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.5}\n",
      "step: (min: 161, avg: 239.2, max: 410\n",
      "{\"metric\": \"step\", \"value\": 239.2}\n",
      "{\"metric\": \"run\", \"value\": 70}\n",
      "score: (min: 0.0, avg: 1.1, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 159, avg: 226.4, max: 391\n",
      "{\"metric\": \"step\", \"value\": 226.4}\n",
      "{\"metric\": \"run\", \"value\": 80}\n",
      "score: (min: 0.0, avg: 1.1, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 165, avg: 222.4, max: 277\n",
      "{\"metric\": \"step\", \"value\": 222.4}\n",
      "{\"metric\": \"run\", \"value\": 90}\n",
      "score: (min: 0.0, avg: 0.7, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 0.7}\n",
      "step: (min: 164, avg: 202.3, max: 271\n",
      "{\"metric\": \"step\", \"value\": 202.3}\n",
      "{\"metric\": \"run\", \"value\": 100}\n",
      "score: (min: 0.0, avg: 1.7, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.7}\n",
      "step: (min: 161, avg: 258.7, max: 353\n",
      "{\"metric\": \"step\", \"value\": 258.7}\n",
      "{\"metric\": \"run\", \"value\": 110}\n",
      "score: (min: 0.0, avg: 1.9, max: 6.0\n",
      "{\"metric\": \"score\", \"value\": 1.9}\n",
      "step: (min: 181, avg: 276.4, max: 541\n",
      "{\"metric\": \"step\", \"value\": 276.4}\n",
      "{\"metric\": \"run\", \"value\": 120}\n",
      "score: (min: 0.0, avg: 1.2, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.2}\n",
      "step: (min: 162, avg: 228.6, max: 304\n",
      "{\"metric\": \"step\", \"value\": 228.6}\n",
      "{\"metric\": \"run\", \"value\": 130}\n",
      "score: (min: 0.0, avg: 1.4, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 176, avg: 247.6, max: 409\n",
      "{\"metric\": \"step\", \"value\": 247.6}\n",
      "{\"metric\": \"run\", \"value\": 140}\n",
      "score: (min: 0.0, avg: 1.0, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 1.0}\n",
      "step: (min: 168, avg: 234.1, max: 325\n",
      "{\"metric\": \"step\", \"value\": 234.1}\n",
      "{\"metric\": \"run\", \"value\": 150}\n",
      "score: (min: 0.0, avg: 1.3, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.3}\n",
      "step: (min: 160, avg: 239.3, max: 356\n",
      "{\"metric\": \"step\", \"value\": 239.3}\n",
      "{\"metric\": \"run\", \"value\": 160}\n",
      "score: (min: 0.0, avg: 0.9, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 0.9}\n",
      "step: (min: 178, avg: 215.5, max: 260\n",
      "{\"metric\": \"step\", \"value\": 215.5}\n",
      "{\"metric\": \"run\", \"value\": 170}\n",
      "score: (min: 0.0, avg: 1.7, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.7}\n",
      "step: (min: 172, avg: 259.7, max: 341\n",
      "{\"metric\": \"step\", \"value\": 259.7}\n",
      "{\"metric\": \"run\", \"value\": 180}\n",
      "score: (min: 0.0, avg: 1.5, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.5}\n",
      "step: (min: 160, avg: 247.3, max: 376\n",
      "{\"metric\": \"step\", \"value\": 247.3}\n",
      "{\"metric\": \"run\", \"value\": 190}\n",
      "score: (min: 0.0, avg: 1.8, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 1.8}\n",
      "step: (min: 175, avg: 268.6, max: 442\n",
      "{\"metric\": \"step\", \"value\": 268.6}\n",
      "{\"metric\": \"run\", \"value\": 200}\n",
      "score: (min: 0.0, avg: 1.9, max: 10.0\n",
      "{\"metric\": \"score\", \"value\": 1.9}\n",
      "step: (min: 169, avg: 262.4, max: 546\n",
      "{\"metric\": \"step\", \"value\": 262.4}\n",
      "{\"metric\": \"run\", \"value\": 210}\n",
      "score: (min: 0.0, avg: 1.5, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.5}\n",
      "step: (min: 168, avg: 244.3, max: 346\n",
      "{\"metric\": \"step\", \"value\": 244.3}\n",
      "{\"metric\": \"run\", \"value\": 220}\n",
      "loss: (min: 0.024123001843690872, avg: 0.039845825031399724, max: 5\n",
      "{\"metric\": \"loss\", \"value\": 0.039845825031399724}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.04118461970239878, avg: 0.05443693601772189, max: 0.16782230746001006\n",
      "{\"metric\": \"q\", \"value\": 0.05443693601772189}\n",
      "score: (min: 0.0, avg: 0.6, max: 1.0\n",
      "{\"metric\": \"score\", \"value\": 0.6}\n",
      "step: (min: 163, avg: 205, max: 237\n",
      "{\"metric\": \"step\", \"value\": 205}\n",
      "{\"metric\": \"run\", \"value\": 230}\n",
      "score: (min: 0.0, avg: 0.6, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 0.6}\n",
      "step: (min: 171, avg: 208.6, max: 279\n",
      "{\"metric\": \"step\", \"value\": 208.6}\n",
      "{\"metric\": \"run\", \"value\": 240}\n",
      "loss: (min: 0.024715745821595192, avg: 0.03377413300797343, max: 0.13560543954372406\n",
      "{\"metric\": \"loss\", \"value\": 0.03377413300797343}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.04122783981263638, avg: 0.05488567421987653, max: 0.16804184526205063\n",
      "{\"metric\": \"q\", \"value\": 0.05488567421987653}\n",
      "score: (min: 0.0, avg: 0.7, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 0.7}\n",
      "step: (min: 139, avg: 205.1, max: 307\n",
      "{\"metric\": \"step\", \"value\": 205.1}\n",
      "{\"metric\": \"run\", \"value\": 250}\n",
      "score: (min: 0.0, avg: 1.5, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.5}\n",
      "step: (min: 143, avg: 247.3, max: 363\n",
      "{\"metric\": \"step\", \"value\": 247.3}\n",
      "{\"metric\": \"run\", \"value\": 260}\n",
      "loss: (min: 0.024986257776618004, avg: 0.033161605421453716, max: 0.15752899646759033\n",
      "{\"metric\": \"loss\", \"value\": 0.033161605421453716}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.04135601755231619, avg: 0.05372447284910828, max: 0.20856564611196518\n",
      "{\"metric\": \"q\", \"value\": 0.05372447284910828}\n",
      "score: (min: 0.0, avg: 0.8, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 0.8}\n",
      "step: (min: 165, avg: 211.7, max: 301\n",
      "{\"metric\": \"step\", \"value\": 211.7}\n",
      "{\"metric\": \"run\", \"value\": 270}\n",
      "loss: (min: 0.025132175534963608, avg: 0.03286105325818062, max: 0.0814291313290596\n",
      "{\"metric\": \"loss\", \"value\": 0.03286105325818062}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.041459301039576534, avg: 0.05332915236789733, max: 0.12410992816090584\n",
      "{\"metric\": \"q\", \"value\": 0.05332915236789733}\n",
      "score: (min: 1.0, avg: 1.7, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.7}\n",
      "step: (min: 208, avg: 257.8, max: 366\n",
      "{\"metric\": \"step\", \"value\": 257.8}\n",
      "{\"metric\": \"run\", \"value\": 280}\n",
      "score: (min: 0.0, avg: 1.9, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.9}\n",
      "step: (min: 174, avg: 273.8, max: 386\n",
      "{\"metric\": \"step\", \"value\": 273.8}\n",
      "{\"metric\": \"run\", \"value\": 290}\n",
      "loss: (min: 0.024980423972010612, avg: 0.03374382232688367, max: 0.13404983282089233\n",
      "{\"metric\": \"loss\", \"value\": 0.03374382232688367}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.041429697163403036, avg: 0.054840532987304035, max: 0.16754865877330302\n",
      "{\"metric\": \"q\", \"value\": 0.054840532987304035}\n",
      "score: (min: 0.0, avg: 1.4, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 166, avg: 242.9, max: 370\n",
      "{\"metric\": \"step\", \"value\": 242.9}\n",
      "{\"metric\": \"run\", \"value\": 300}\n",
      "loss: (min: 0.024910328909754753, avg: 0.033163216842338444, max: 0.08022691309452057\n",
      "{\"metric\": \"loss\", \"value\": 0.033163216842338444}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.04122792832553387, avg: 0.05411072087291628, max: 0.12136035613715648\n",
      "{\"metric\": \"q\", \"value\": 0.05411072087291628}\n",
      "score: (min: 0.0, avg: 1.4, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 169, avg: 253.5, max: 393\n",
      "{\"metric\": \"step\", \"value\": 253.5}\n",
      "{\"metric\": \"run\", \"value\": 310}\n",
      "score: (min: 0.0, avg: 1.0, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.0}\n",
      "step: (min: 163, avg: 222, max: 358\n",
      "{\"metric\": \"step\", \"value\": 222}\n",
      "{\"metric\": \"run\", \"value\": 320}\n",
      "loss: (min: 0.024704622104763985, avg: 0.033386555107310416, max: 0.1381359100341797\n",
      "{\"metric\": \"loss\", \"value\": 0.033386555107310416}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.04066641356796026, avg: 0.054513581412285565, max: 0.1782497964799404\n",
      "{\"metric\": \"q\", \"value\": 0.054513581412285565}\n",
      "score: (min: 0.0, avg: 1.6, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.6}\n",
      "step: (min: 159, avg: 258, max: 396\n",
      "{\"metric\": \"step\", \"value\": 258}\n",
      "{\"metric\": \"run\", \"value\": 330}\n",
      "{\"metric\": \"epsilon\", \"value\": 0.9682342352941838}\n",
      "{\"metric\": \"total_step\", \"value\": 80000}\n",
      "score: (min: 0.0, avg: 0.7, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 0.7}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: (min: 164, avg: 211.1, max: 324\n",
      "{\"metric\": \"step\", \"value\": 211.1}\n",
      "{\"metric\": \"run\", \"value\": 340}\n",
      "loss: (min: 7.540489605162293e-05, avg: 0.017999955053193843, max: 0.07139038294553757\n",
      "{\"metric\": \"loss\", \"value\": 0.017999955053193843}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.0020475370483472944, avg: 0.03111451300702058, max: 0.12134831100702285\n",
      "{\"metric\": \"q\", \"value\": 0.03111451300702058}\n",
      "score: (min: 0.0, avg: 2.2, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 2.2}\n",
      "step: (min: 169, avg: 287.6, max: 349\n",
      "{\"metric\": \"step\", \"value\": 287.6}\n",
      "{\"metric\": \"run\", \"value\": 350}\n",
      "loss: (min: 5.9763246099464595e-05, avg: 0.0025630934903674643, max: 0.10897110402584076\n",
      "{\"metric\": \"loss\", \"value\": 0.0025630934903674643}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.0020224187476560475, avg: 0.007848593778738287, max: 0.12735035073477774\n",
      "{\"metric\": \"q\", \"value\": 0.007848593778738287}\n",
      "score: (min: 0.0, avg: 1.5, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 1.5}\n",
      "step: (min: 165, avg: 253.8, max: 435\n",
      "{\"metric\": \"step\", \"value\": 253.8}\n",
      "{\"metric\": \"run\", \"value\": 360}\n",
      "score: (min: 0.0, avg: 1.6, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.6}\n",
      "step: (min: 146, avg: 252.7, max: 431\n",
      "{\"metric\": \"step\", \"value\": 252.7}\n",
      "{\"metric\": \"run\", \"value\": 370}\n",
      "loss: (min: 6.943500193301588e-05, avg: 0.002738621097312716, max: 0.03013714775443077\n",
      "{\"metric\": \"loss\", \"value\": 0.002738621097312716}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.0020198970520868896, avg: 0.008298603264561388, max: 0.06779558651149273\n",
      "{\"metric\": \"q\", \"value\": 0.008298603264561388}\n",
      "score: (min: 0.0, avg: 1.5, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.5}\n",
      "step: (min: 162, avg: 247.6, max: 363\n",
      "{\"metric\": \"step\", \"value\": 247.6}\n",
      "{\"metric\": \"run\", \"value\": 380}\n",
      "score: (min: 0.0, avg: 1.1, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 164, avg: 236.5, max: 350\n",
      "{\"metric\": \"step\", \"value\": 236.5}\n",
      "{\"metric\": \"run\", \"value\": 390}\n",
      "loss: (min: 6.0167225456098095e-05, avg: 0.0027563875218365864, max: 0.030408214777708054\n",
      "{\"metric\": \"loss\", \"value\": 0.0027563875218365864}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 0.001995414476841688, avg: 0.008323364191260188, max: 0.06614299447275698\n",
      "{\"metric\": \"q\", \"value\": 0.008323364191260188}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-46f1145dd9a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgame_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDQNTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_SHAPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_step_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_run_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-a5b3228ea66a>\u001b[0m in \u001b[0;36mmain_loop\u001b[0;34m(game_model, env, render, total_step_limit, total_run_limit, clip)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mgame_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-97647831e52a>\u001b[0m in \u001b[0;36mstep_update\u001b[0;34m(self, total_step)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mTRAINING_FREQUENCY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_max_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-97647831e52a>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"current_state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mcurrent_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"next_state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game_model = DDQNTrainer(game_mode, INPUT_SHAPE, env.action_space.n)\n",
    "main_loop(game_model, env, render, total_step_limit, total_run_limit, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNTrainer:\n",
    "    def __init__(self, game_name, input_shape, action_space):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.game_name = game_name\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.game_mode = \"DDQN training\"\n",
    "        self.model_path = \"./output/neural_nets/\" + game_name + \"/ddqn/\" + self._get_date() + \"/model.h5\"\n",
    "        self.logger_path = \"./output/logs/\" + game_name + \"/ddqn/training/\" + self._get_date() + \"/\"\n",
    "        self.logger = Logger(self.game_name + \" \" + self.game_mode, self.logger_path)\n",
    "        \n",
    "        if os.path.exists(os.path.dirname(self.model_path)):\n",
    "            shutil.rmtree(os.path.dirname(self.model_path), ignore_errors=True)\n",
    "        os.makedirs(os.path.dirname(self.model_path))\n",
    "\n",
    "        self.ddqn = ConvolutionalNeuralNetwork(self.input_shape, action_space).to(device)\n",
    "        \n",
    "        self.criteria = nn.MSELoss()\n",
    "        self.optimizer = RMSprop(self.ddqn.parameters(), lr=0.00025, alpha = 0.95, eps=0.01)\n",
    "        \n",
    "        if os.path.isfile(self.model_path):\n",
    "            self.ddqn.load_state_dict(torch.load(self.model_path))\n",
    "        \n",
    "        self.ddqn_target = ConvolutionalNeuralNetwork(self.input_shape, action_space).to(device)\n",
    "#         .model\n",
    "        self._reset_target_network()\n",
    "        self.epsilon = EXPLORATION_MAX\n",
    "        self.memory = []\n",
    "    \n",
    "    def _save_model(self):\n",
    "        torch.save(self.ddqn.state_dict(),self.model_path)\n",
    "#         self.ddqn.save_weights(self.model_path)\n",
    "        \n",
    "    def _get_date(self):\n",
    "        return str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "        \n",
    "    def save_run(self, score, step, run):\n",
    "        self.logger.add_score(score)\n",
    "        self.logger.add_step(step)\n",
    "        self.logger.add_run(run)\n",
    "\n",
    "    def move(self, state):\n",
    "        if np.random.rand() < self.epsilon or len(self.memory) < REPLAY_START_SIZE:\n",
    "            return random.randrange(self.action_space)\n",
    "        \n",
    "        q_values = self.ddqn(np.expand_dims(np.asarray(state).astype(np.float64), axis=0), batch_size=1)\n",
    "        return np.argmax(q_values[0].detach().cpu().numpy())\n",
    "\n",
    "    def remember(self, current_state, action, reward, next_state, terminal):\n",
    "        self.memory.append({\"current_state\": current_state,\n",
    "                            \"action\": action,\n",
    "                            \"reward\": reward,\n",
    "                            \"next_state\": next_state,\n",
    "                            \"terminal\": terminal})\n",
    "        if len(self.memory) > MEMORY_SIZE:\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def step_update(self, total_step):\n",
    "        if len(self.memory) < REPLAY_START_SIZE:\n",
    "            return\n",
    "\n",
    "        if total_step % TRAINING_FREQUENCY == 0:\n",
    "            loss, accuracy, average_max_q = self._train()\n",
    "            self.logger.add_loss(loss.item())\n",
    "            self.logger.add_accuracy(accuracy)\n",
    "            self.logger.add_q(average_max_q)\n",
    "\n",
    "        self._update_epsilon()\n",
    "\n",
    "        if total_step % MODEL_PERSISTENCE_UPDATE_FREQUENCY == 0:\n",
    "            self._save_model()\n",
    "\n",
    "        if total_step % TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "            self._reset_target_network()\n",
    "            print('{{\"metric\": \"epsilon\", \"value\": {}}}'.format(self.epsilon))\n",
    "            print('{{\"metric\": \"total_step\", \"value\": {}}}'.format(total_step))\n",
    "\n",
    "    def _train(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        batch = np.asarray(random.sample(self.memory, BATCH_SIZE))\n",
    "        if len(batch) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        current_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        terminals = []\n",
    "        \n",
    "        q_values = []\n",
    "        max_q_values = []\n",
    "        \n",
    "#         print(batch)\n",
    "        for entry in batch:\n",
    "            current_state = np.expand_dims(np.asarray(entry[\"current_state\"]).astype(np.float64), axis=0)\n",
    "            current_states.append(current_state)\n",
    "            next_state = np.expand_dims(np.asarray(entry[\"next_state\"]).astype(np.float64), axis=0)\n",
    "            \n",
    "            next_states.append(next_state)\n",
    "            rewards.append(entry['reward'])\n",
    "            actions.append(entry['action'])\n",
    "            terminals.append(entry['terminal'])\n",
    "            \n",
    "        non_final_mask = torch.tensor(terminals)==False\n",
    "        non_final_next_states = np.asarray(next_states).squeeze()\n",
    "        \n",
    "        state_batch = np.asarray(current_states).squeeze()\n",
    "        action_batch = np.asanyarray(actions).squeeze()\n",
    "        reward_batch = np.asarray(rewards).squeeze()\n",
    "#         print(torch.from_numpy(action_batch).unsqueeze(dim=1).size(),torch.Size([32, 1]))\n",
    "        state_action_values = self.ddqn(state_batch).gather(1, torch.from_numpy(action_batch).unsqueeze(dim=1).to(device))\n",
    "    \n",
    "#         next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        next_state_values = np.zeros(BATCH_SIZE)\n",
    "        next_state_values[non_final_mask] = np.max(self.ddqn_target(non_final_next_states).detach().cpu().numpy())\n",
    "#         .max(1)[0].detach().cpu()    \n",
    "    \n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "        \n",
    "        loss = self.criteria(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "#         print(state_action_values.size(),[32, 4])\n",
    "#         print(state_action_values.size(),torch.Size([32, 1]))\n",
    "#         print(non_final_next_states.squeeze().size(), torch.Size([32, 4, 84, 84]))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "#             next_state_prediction = self.ddqn_target(next_state).detach().cpu().numpy().ravel()\n",
    "# #             print(next_state_prediction.shape, (4,))\n",
    "            \n",
    "#             next_q_value = np.max(next_state_prediction)\n",
    "#             q = list(self.ddqn(current_state)[0].detach().cpu().numpy())\n",
    "# #             print(len(q), 4)\n",
    "#             if entry[\"terminal\"]:\n",
    "#                 q[entry[\"action\"]] = entry[\"reward\"]\n",
    "#             else:\n",
    "#                 q[entry[\"action\"]] = entry[\"reward\"] + GAMMA * next_q_value\n",
    "#             q_values.append(q)\n",
    "#             max_q_values.append(np.max(q))\n",
    "        \n",
    "#         model_out = self.ddqn(np.asarray(current_states).squeeze())\n",
    "#         loss = self.criteria(model_out, torch.from_numpy(np.array(q_values)).float().to(device))\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "    \n",
    "\n",
    "        accuracy = 0\n",
    "        return loss, accuracy, mean(max_q_values)\n",
    "\n",
    "    def _update_epsilon(self):\n",
    "        self.epsilon -= EXPLORATION_DECAY\n",
    "        self.epsilon = max(EXPLORATION_MIN, self.epsilon)\n",
    "\n",
    "    def _reset_target_network(self):\n",
    "        self.ddqn_target.load_state_dict(self.ddqn.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
