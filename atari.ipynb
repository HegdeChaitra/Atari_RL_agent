{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import itertools\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import argparse\n",
    "from torch import optim\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "from torch.utils.data import Sampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import seaborn\n",
    "import datetime\n",
    "from logger import Logger\n",
    "\n",
    "from torch.optim import RMSprop\n",
    "import shutil\n",
    "from statistics import mean\n",
    "\n",
    "from gym_wrappers import MainGymWrapper\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, action_space):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.action_space = action_space\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=8, stride=(4,4))\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64,kernel_size=4,stride=(2,2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        o_size = 3136\n",
    "        self.linear1 = nn.Linear(in_features=o_size, out_features=512)\n",
    "        self.linear2 = nn.Linear(in_features=512, out_features=self.action_space)\n",
    "        \n",
    "    def forward(self, x, batch_size=None):\n",
    "        x = torch.from_numpy(x).float()\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "\n",
    "        # out = (bs, nc, x, y)\n",
    "        if batch_size==None:\n",
    "            batch_size = out.size(0)\n",
    "#         print(out.size())\n",
    "        out_flat = out.view(batch_size, -1)\n",
    "        \n",
    "        out_flat = F.relu(self.linear1(out_flat))\n",
    "        \n",
    "        out_flat = self.linear2(out_flat)\n",
    "        return out_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = \"Breakout-v0\"\n",
    "game_mode = 'ddqn_training'\n",
    "render = False\n",
    "total_step_limit = 5000000\n",
    "total_run_limit = None\n",
    "clip = True\n",
    "\n",
    "FRAMES_IN_OBSERVATION = 4\n",
    "FRAME_SIZE = 84\n",
    "INPUT_SHAPE = (FRAMES_IN_OBSERVATION, FRAME_SIZE, FRAME_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Breakout-v0', 'Breakout-v0Deterministic-v4')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = game_name + \"Deterministic-v4\" \n",
    "game_name, env_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = MainGymWrapper.wrap(gym.make(game_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "MEMORY_SIZE = 900000\n",
    "BATCH_SIZE = 32\n",
    "TRAINING_FREQUENCY = 4\n",
    "TARGET_NETWORK_UPDATE_FREQUENCY = 40000\n",
    "MODEL_PERSISTENCE_UPDATE_FREQUENCY = 10000\n",
    "REPLAY_START_SIZE = 50000\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.1\n",
    "EXPLORATION_TEST = 0.02\n",
    "EXPLORATION_STEPS = 850000\n",
    "EXPLORATION_DECAY = (EXPLORATION_MAX-EXPLORATION_MIN)/EXPLORATION_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNTrainer:\n",
    "    def __init__(self, game_name, input_shape, action_space):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.game_name = game_name\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.game_mode = \"DDQN training\"\n",
    "        self.model_path = \"./output/neural_nets/\" + game_name + \"/ddqn/\" + self._get_date() + \"/model.h5\"\n",
    "        self.logger_path = \"./output/logs/\" + game_name + \"/ddqn/training/\" + self._get_date() + \"/\"\n",
    "        self.logger = Logger(self.game_name + \" \" + self.game_mode, self.logger_path)\n",
    "        \n",
    "        if os.path.exists(os.path.dirname(self.model_path)):\n",
    "            shutil.rmtree(os.path.dirname(self.model_path), ignore_errors=True)\n",
    "        os.makedirs(os.path.dirname(self.model_path))\n",
    "\n",
    "        self.ddqn = ConvolutionalNeuralNetwork(self.input_shape, action_space)\n",
    "        \n",
    "        self.criteria = nn.CrossEntropyLoss()\n",
    "        self.optimizer = RMSprop(self.ddqn.parameters(), lr=0.00025, alpha = 0.95, eps=0.01)\n",
    "        \n",
    "        if os.path.isfile(self.model_path):\n",
    "            self.ddqn.load_weights(self.model_path)\n",
    "        \n",
    "        self.ddqn_target = ConvolutionalNeuralNetwork(self.input_shape, action_space)\n",
    "#         .model\n",
    "        self._reset_target_network()\n",
    "        self.epsilon = EXPLORATION_MAX\n",
    "        self.memory = []\n",
    "    \n",
    "    def _save_model(self):\n",
    "        self.ddqn.save_weights(self.model_path)\n",
    "        \n",
    "    def _get_date(self):\n",
    "        return str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "        \n",
    "    def save_run(self, score, step, run):\n",
    "        self.logger.add_score(score)\n",
    "        self.logger.add_step(step)\n",
    "        self.logger.add_run(run)\n",
    "\n",
    "    def move(self, state):\n",
    "        if np.random.rand() < self.epsilon or len(self.memory) < REPLAY_START_SIZE:\n",
    "            return random.randrange(self.action_space)\n",
    "        \n",
    "        q_values = self.ddqn(np.expand_dims(np.asarray(state).astype(np.float64), axis=0), batch_size=1)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def remember(self, current_state, action, reward, next_state, terminal):\n",
    "        self.memory.append({\"current_state\": current_state,\n",
    "                            \"action\": action,\n",
    "                            \"reward\": reward,\n",
    "                            \"next_state\": next_state,\n",
    "                            \"terminal\": terminal})\n",
    "        if len(self.memory) > MEMORY_SIZE:\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def step_update(self, total_step):\n",
    "        if len(self.memory) < REPLAY_START_SIZE:\n",
    "            return\n",
    "\n",
    "        if total_step % TRAINING_FREQUENCY == 0:\n",
    "            loss, accuracy, average_max_q = self._train()\n",
    "            self.logger.add_loss(loss)\n",
    "            self.logger.add_accuracy(accuracy)\n",
    "            self.logger.add_q(average_max_q)\n",
    "\n",
    "        self._update_epsilon()\n",
    "\n",
    "        if total_step % MODEL_PERSISTENCE_UPDATE_FREQUENCY == 0:\n",
    "            self._save_model()\n",
    "\n",
    "        if total_step % TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "            self._reset_target_network()\n",
    "            print('{{\"metric\": \"epsilon\", \"value\": {}}}'.format(self.epsilon))\n",
    "            print('{{\"metric\": \"total_step\", \"value\": {}}}'.format(total_step))\n",
    "\n",
    "    def _train(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        batch = np.asarray(random.sample(self.memory, BATCH_SIZE))\n",
    "        if len(batch) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        current_states = []\n",
    "        q_values = []\n",
    "        max_q_values = []\n",
    "\n",
    "        for entry in batch:\n",
    "            current_state = np.expand_dims(np.asarray(entry[\"current_state\"]).astype(np.float64), axis=0)\n",
    "            current_states.append(current_state)\n",
    "            next_state = np.expand_dims(np.asarray(entry[\"next_state\"]).astype(np.float64), axis=0)\n",
    "            \n",
    "            next_state_prediction = self.ddqn_target(next_state).detach().numpy().ravel()\n",
    "            \n",
    "            next_q_value = np.max(next_state_prediction)\n",
    "            q = list(self.ddqn(current_state)[0].detach().numpy())\n",
    "            \n",
    "            if entry[\"terminal\"]:\n",
    "                q[entry[\"action\"]] = entry[\"reward\"]\n",
    "            else:\n",
    "                q[entry[\"action\"]] = entry[\"reward\"] + GAMMA * next_q_value\n",
    "            q_values.append(q)\n",
    "            max_q_values.append(np.max(q))\n",
    "        \n",
    "        model_out = self.ddqn(np.asarray(current_states).squeeze())\n",
    "        \n",
    "#         fit = self.ddqn.fit(np.asarray(current_states).squeeze(),\n",
    "#                             np.asarray(q_values).squeeze(),\n",
    "#                             batch_size=BATCH_SIZE,\n",
    "#                             verbose=0)\n",
    "\n",
    "        #TODO : Check proper input to loss function\n",
    "        loss = self.criteria(model_out, q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "\n",
    "#         loss = fit.history[\"loss\"][0]\n",
    "#         accuracy = fit.history[\"acc\"][0]\n",
    "        return loss, accuracy, mean(max_q_values)\n",
    "\n",
    "    def _update_epsilon(self):\n",
    "        self.epsilon -= EXPLORATION_DECAY\n",
    "        self.epsilon = max(EXPLORATION_MIN, self.epsilon)\n",
    "\n",
    "    def _reset_target_network(self):\n",
    "        self.ddqn_target.load_state_dict(self.ddqn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(game_model, env, render, total_step_limit, total_run_limit, clip):\n",
    "    run = 0\n",
    "    total_step = 0\n",
    "    while True:\n",
    "        if total_run_limit is not None and run >= total_run_limit:\n",
    "            print(\"Reached total run limit of: \" + str(total_run_limit))\n",
    "            exit(0)\n",
    "\n",
    "        run += 1\n",
    "        current_state = env.reset()\n",
    "        step = 0\n",
    "        score = 0\n",
    "        while True:\n",
    "            if total_step >= total_step_limit:\n",
    "                print(\"Reached total step limit of: \" + str(total_step_limit))\n",
    "                exit(0)\n",
    "            total_step += 1\n",
    "            step += 1\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            action = game_model.move(current_state)\n",
    "            next_state, reward, terminal, info = env.step(action)\n",
    "            if clip:\n",
    "                np.sign(reward)\n",
    "            score += reward\n",
    "            game_model.remember(current_state, action, reward, next_state, terminal)\n",
    "            current_state = next_state\n",
    "\n",
    "            game_model.step_update(total_step)\n",
    "\n",
    "            if terminal:\n",
    "                game_model.save_run(score, step, run)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: (min: 0.0, avg: 1.4, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 174, avg: 252.4, max: 366\n",
      "{\"metric\": \"step\", \"value\": 252.4}\n",
      "{\"metric\": \"run\", \"value\": 10}\n",
      "score: (min: 0.0, avg: 0.9, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 0.9}\n",
      "step: (min: 168, avg: 220.8, max: 342\n",
      "{\"metric\": \"step\", \"value\": 220.8}\n",
      "{\"metric\": \"run\", \"value\": 20}\n",
      "score: (min: 0.0, avg: 1.5, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.5}\n",
      "step: (min: 161, avg: 251.9, max: 385\n",
      "{\"metric\": \"step\", \"value\": 251.9}\n",
      "{\"metric\": \"run\", \"value\": 30}\n",
      "score: (min: 0.0, avg: 0.8, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 0.8}\n",
      "step: (min: 167, avg: 221.9, max: 298\n",
      "{\"metric\": \"step\", \"value\": 221.9}\n",
      "{\"metric\": \"run\", \"value\": 40}\n",
      "score: (min: 0.0, avg: 1.8, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.8}\n",
      "step: (min: 164, avg: 264.9, max: 397\n",
      "{\"metric\": \"step\", \"value\": 264.9}\n",
      "{\"metric\": \"run\", \"value\": 50}\n",
      "score: (min: 0.0, avg: 1.4, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 169, avg: 252.3, max: 401\n",
      "{\"metric\": \"step\", \"value\": 252.3}\n",
      "{\"metric\": \"run\", \"value\": 60}\n",
      "score: (min: 0.0, avg: 1.3, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.3}\n",
      "step: (min: 159, avg: 229, max: 361\n",
      "{\"metric\": \"step\", \"value\": 229}\n",
      "{\"metric\": \"run\", \"value\": 70}\n",
      "score: (min: 0.0, avg: 1.4, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 170, avg: 253.3, max: 419\n",
      "{\"metric\": \"step\", \"value\": 253.3}\n",
      "{\"metric\": \"run\", \"value\": 80}\n",
      "score: (min: 0.0, avg: 1.4, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 165, avg: 241.4, max: 334\n",
      "{\"metric\": \"step\", \"value\": 241.4}\n",
      "{\"metric\": \"run\", \"value\": 90}\n",
      "score: (min: 0.0, avg: 1.1, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 162, avg: 236.6, max: 389\n",
      "{\"metric\": \"step\", \"value\": 236.6}\n",
      "{\"metric\": \"run\", \"value\": 100}\n",
      "score: (min: 0.0, avg: 1.7, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.7}\n",
      "step: (min: 171, avg: 269.2, max: 412\n",
      "{\"metric\": \"step\", \"value\": 269.2}\n",
      "{\"metric\": \"run\", \"value\": 110}\n",
      "score: (min: 0.0, avg: 0.5, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 0.5}\n",
      "step: (min: 160, avg: 206.5, max: 330\n",
      "{\"metric\": \"step\", \"value\": 206.5}\n",
      "{\"metric\": \"run\", \"value\": 120}\n",
      "score: (min: 0.0, avg: 1.0, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.0}\n",
      "step: (min: 159, avg: 219.2, max: 341\n",
      "{\"metric\": \"step\", \"value\": 219.2}\n",
      "{\"metric\": \"run\", \"value\": 130}\n",
      "score: (min: 0.0, avg: 0.9, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 0.9}\n",
      "step: (min: 144, avg: 214.8, max: 338\n",
      "{\"metric\": \"step\", \"value\": 214.8}\n",
      "{\"metric\": \"run\", \"value\": 140}\n",
      "score: (min: 0.0, avg: 1.3, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 1.3}\n",
      "step: (min: 167, avg: 240.1, max: 428\n",
      "{\"metric\": \"step\", \"value\": 240.1}\n",
      "{\"metric\": \"run\", \"value\": 150}\n",
      "score: (min: 0.0, avg: 1.1, max: 6.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 159, avg: 207.2, max: 337\n",
      "{\"metric\": \"step\", \"value\": 207.2}\n",
      "{\"metric\": \"run\", \"value\": 160}\n",
      "score: (min: 0.0, avg: 1.8, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 1.8}\n",
      "step: (min: 173, avg: 262, max: 441\n",
      "{\"metric\": \"step\", \"value\": 262}\n",
      "{\"metric\": \"run\", \"value\": 170}\n",
      "score: (min: 0.0, avg: 0.9, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 0.9}\n",
      "step: (min: 160, avg: 211.6, max: 281\n",
      "{\"metric\": \"step\", \"value\": 211.6}\n",
      "{\"metric\": \"run\", \"value\": 180}\n",
      "score: (min: 0.0, avg: 1.3, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.3}\n",
      "step: (min: 163, avg: 235.3, max: 372\n",
      "{\"metric\": \"step\", \"value\": 235.3}\n",
      "{\"metric\": \"run\", \"value\": 190}\n",
      "score: (min: 0.0, avg: 1.2, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.2}\n",
      "step: (min: 142, avg: 242.4, max: 331\n",
      "{\"metric\": \"step\", \"value\": 242.4}\n",
      "{\"metric\": \"run\", \"value\": 200}\n",
      "score: (min: 0.0, avg: 1.7, max: 6.0\n",
      "{\"metric\": \"score\", \"value\": 1.7}\n",
      "step: (min: 164, avg: 250.9, max: 410\n",
      "{\"metric\": \"step\", \"value\": 250.9}\n",
      "{\"metric\": \"run\", \"value\": 210}\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([32, 64, 7, 7])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-46f1145dd9a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgame_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDQNTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_SHAPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_step_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_run_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-131-a5b3228ea66a>\u001b[0m in \u001b[0;36mmain_loop\u001b[0;34m(game_model, env, render, total_step_limit, total_run_limit, clip)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mgame_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-130-8e6477152b87>\u001b[0m in \u001b[0;36mstep_update\u001b[0;34m(self, total_step)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mTRAINING_FREQUENCY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_max_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-130-8e6477152b87>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m#TODO : Check proper input to loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriteria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 862\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected 2 or more dimensions (got {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[1;32m   1405\u001b[0m                          .format(input.size(0), target.size(0)))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "game_model = DDQNTrainer(game_mode, INPUT_SHAPE, env.action_space.n)\n",
    "main_loop(game_model, env, render, total_step_limit, total_run_limit, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
