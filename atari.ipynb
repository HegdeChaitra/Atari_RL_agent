{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/atari_game/logger.py:7: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/traitlets-4.3.2-py3.6.egg/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 112, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/asyncio/base_events.py\", line 1426, in _run_once\n",
      "    handle._run()\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/asyncio/events.py\", line 127, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 102, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/ipython-6.2.1-py3.6.egg/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/ipython-6.2.1-py3.6.egg/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/ipython-6.2.1-py3.6.egg/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-13425d297397>\", line 32, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/matplotlib-2.1.0-py3.6-linux-x86_64.egg/matplotlib/pyplot.py\", line 69, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/matplotlib-2.1.0-py3.6-linux-x86_64.egg/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use(\"Agg\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import itertools\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import argparse\n",
    "from torch import optim\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "from torch.utils.data import Sampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import seaborn\n",
    "import datetime\n",
    "from logger import Logger\n",
    "\n",
    "from torch.optim import RMSprop\n",
    "import shutil\n",
    "from statistics import mean\n",
    "\n",
    "from gym_wrappers import MainGymWrapper\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, action_space):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.action_space = action_space\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=8, stride=(4,4))\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64,kernel_size=4,stride=(2,2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        o_size = 3136\n",
    "        self.linear1 = nn.Linear(in_features=o_size, out_features=512)\n",
    "        self.linear2 = nn.Linear(in_features=512, out_features=self.action_space)\n",
    "        \n",
    "    def forward(self, x, batch_size=None):\n",
    "        x = torch.from_numpy(x).float().to(device)\n",
    "#         print(x.size(), torch.Size([1, 4, 84, 84]))\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "\n",
    "        # out = (bs, nc, x, y)\n",
    "        if batch_size==None:\n",
    "            batch_size = out.size(0)\n",
    "#         print(out.size())\n",
    "        out_flat = out.view(batch_size, -1)\n",
    "        \n",
    "        out_flat = F.relu(self.linear1(out_flat))\n",
    "        \n",
    "        out_flat = self.linear2(out_flat)\n",
    "        return out_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = \"Breakout-v0\"\n",
    "game_mode = 'ddqn_training'\n",
    "render = False\n",
    "total_step_limit = 5000000\n",
    "total_run_limit = None\n",
    "clip = True\n",
    "\n",
    "FRAMES_IN_OBSERVATION = 4\n",
    "FRAME_SIZE = 84\n",
    "INPUT_SHAPE = (FRAMES_IN_OBSERVATION, FRAME_SIZE, FRAME_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Breakout-v0', 'Breakout-v0Deterministic-v4')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = game_name + \"Deterministic-v4\" \n",
    "game_name, env_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = MainGymWrapper.wrap(gym.make(game_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "MEMORY_SIZE = 900000\n",
    "BATCH_SIZE = 16000\n",
    "TRAINING_FREQUENCY = 4\n",
    "TARGET_NETWORK_UPDATE_FREQUENCY = 40000\n",
    "MODEL_PERSISTENCE_UPDATE_FREQUENCY = 10000\n",
    "REPLAY_START_SIZE = 50000\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.1\n",
    "EXPLORATION_TEST = 0.02\n",
    "EXPLORATION_STEPS = 850000\n",
    "EXPLORATION_DECAY = (EXPLORATION_MAX-EXPLORATION_MIN)/EXPLORATION_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNTrainer:\n",
    "    def __init__(self, game_name, input_shape, action_space):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.game_name = game_name\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.game_mode = \"DDQN training\"\n",
    "        self.model_path = \"./output/neural_nets/\" + game_name + \"/ddqn/\" + self._get_date() + \"/model.h5\"\n",
    "        self.logger_path = \"./output/logs/\" + game_name + \"/ddqn/training/\" + self._get_date() + \"/\"\n",
    "        self.logger = Logger(self.game_name + \" \" + self.game_mode, self.logger_path)\n",
    "        \n",
    "        if os.path.exists(os.path.dirname(self.model_path)):\n",
    "            shutil.rmtree(os.path.dirname(self.model_path), ignore_errors=True)\n",
    "        os.makedirs(os.path.dirname(self.model_path))\n",
    "\n",
    "        self.ddqn = ConvolutionalNeuralNetwork(self.input_shape, action_space).to(device)\n",
    "        \n",
    "        self.criteria = nn.MSELoss()\n",
    "        self.optimizer = RMSprop(self.ddqn.parameters(), lr=0.00025, alpha = 0.95, eps=0.01)\n",
    "        \n",
    "        if os.path.isfile(self.model_path):\n",
    "            self.ddqn.load_state_dict(torch.load(self.model_path))\n",
    "        \n",
    "        self.ddqn_target = ConvolutionalNeuralNetwork(self.input_shape, action_space).to(device)\n",
    "#         .model\n",
    "        self._reset_target_network()\n",
    "        self.epsilon = EXPLORATION_MAX\n",
    "        self.memory = []\n",
    "    \n",
    "    def _save_model(self):\n",
    "        torch.save(self.ddqn.state_dict(),self.model_path)\n",
    "        \n",
    "    def _get_date(self):\n",
    "        return str(datetime.datetime.now().strftime('%Y-%m-%d'))\n",
    "        \n",
    "    def save_run(self, score, step, run):\n",
    "        self.logger.add_score(score)\n",
    "        self.logger.add_step(step)\n",
    "        self.logger.add_run(run)\n",
    "\n",
    "    def move(self, state):\n",
    "        if np.random.rand() < self.epsilon or len(self.memory) < REPLAY_START_SIZE:\n",
    "            return random.randrange(self.action_space)\n",
    "        \n",
    "        q_values = self.ddqn(np.expand_dims(np.asarray(state).astype(np.float64), axis=0), batch_size=1)\n",
    "        return np.argmax(q_values[0].detach().cpu().numpy())\n",
    "\n",
    "    def remember(self, current_state, action, reward, next_state, terminal):\n",
    "        self.memory.append({\"current_state\": current_state,\n",
    "                            \"action\": action,\n",
    "                            \"reward\": reward,\n",
    "                            \"next_state\": next_state,\n",
    "                            \"terminal\": terminal})\n",
    "        if len(self.memory) > MEMORY_SIZE:\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def step_update(self, total_step):\n",
    "        if len(self.memory) < REPLAY_START_SIZE:\n",
    "            return\n",
    "\n",
    "        if total_step % TRAINING_FREQUENCY == 0:\n",
    "            loss, accuracy, average_max_q = self._train()\n",
    "            self.logger.add_loss(loss.item())\n",
    "            self.logger.add_accuracy(accuracy)\n",
    "            self.logger.add_q(average_max_q)\n",
    "\n",
    "        self._update_epsilon()\n",
    "\n",
    "        if total_step % MODEL_PERSISTENCE_UPDATE_FREQUENCY == 0:\n",
    "            self._save_model()\n",
    "\n",
    "        if total_step % TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "            self._reset_target_network()\n",
    "            print('{{\"metric\": \"epsilon\", \"value\": {}}}'.format(self.epsilon))\n",
    "            print('{{\"metric\": \"total_step\", \"value\": {}}}'.format(total_step))\n",
    "\n",
    "    def _train(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        self.ddqn_target.eval()\n",
    "        batch = np.asarray(random.sample(self.memory, BATCH_SIZE))\n",
    "        if len(batch) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        current_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        terminals = []\n",
    "        \n",
    "        for entry in batch:\n",
    "            current_state = np.expand_dims(np.asarray(entry[\"current_state\"]).astype(np.float64), axis=0)\n",
    "            current_states.append(current_state)\n",
    "            next_state = np.expand_dims(np.asarray(entry[\"next_state\"]).astype(np.float64), axis=0)\n",
    "            \n",
    "            next_states.append(next_state)\n",
    "            rewards.append(entry['reward'])\n",
    "            actions.append(entry['action'])\n",
    "            terminals.append(entry['terminal'])\n",
    "            \n",
    "        non_final_mask = torch.tensor(terminals)==False\n",
    "        non_final_next_states = np.asarray(next_states).squeeze()\n",
    "        \n",
    "        state_batch = np.asarray(current_states).squeeze()\n",
    "        action_batch = np.asanyarray(actions).squeeze()\n",
    "        reward_batch = np.asarray(rewards).squeeze()\n",
    "        state_action_values = self.ddqn(state_batch).gather(1, torch.from_numpy(action_batch).unsqueeze(dim=1).to(device))\n",
    "    \n",
    "        next_state_values = np.zeros(BATCH_SIZE)\n",
    "        temp = np.max(self.ddqn_target(non_final_next_states).detach().cpu().numpy(),axis=1)\n",
    "        \n",
    "        \n",
    "        next_state_values[list(np.array(terminals)==False)] = temp[list(np.array(terminals)==False)]\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "        \n",
    "#         loss = self.criteria(state_action_values, torch.from_numpy(expected_state_action_values).unsqueeze(1))\n",
    "        loss = F.smooth_l1_loss(state_action_values, torch.from_numpy(expected_state_action_values).unsqueeze(1).to(device).float())\n",
    "        loss.backward()\n",
    "        \n",
    "        for param in self.ddqn.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "#         print(torch.from_numpy(action_batch).unsqueeze(dim=1).size(),torch.Size([32, 1]))\n",
    "#         print(state_action_values.size(),[32, 4])\n",
    "#         print(state_action_values.size(),torch.Size([32, 1]))\n",
    "#         print(non_final_next_states.squeeze().size(), torch.Size([32, 4, 84, 84]))\n",
    "        \n",
    "        accuracy = 0\n",
    "        return loss, accuracy, mean(expected_state_action_values)\n",
    "\n",
    "    def _update_epsilon(self):\n",
    "        self.epsilon -= EXPLORATION_DECAY\n",
    "        self.epsilon = max(EXPLORATION_MIN, self.epsilon)\n",
    "\n",
    "    def _reset_target_network(self):\n",
    "        self.ddqn_target.load_state_dict(self.ddqn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(game_model, env, render, total_step_limit, total_run_limit, clip):\n",
    "    run = 0\n",
    "    total_step = 0\n",
    "    while True:\n",
    "        if run%10==0:\n",
    "            print('-'*100)\n",
    "            print(\"RUN = \",run)\n",
    "        if total_run_limit is not None and run >= total_run_limit:\n",
    "            print(\"Reached total run limit of: \" + str(total_run_limit))\n",
    "            exit(0)\n",
    "\n",
    "        run += 1\n",
    "        current_state = env.reset()\n",
    "        step = 0\n",
    "        score = 0\n",
    "\n",
    "        while True:\n",
    "            if total_step >= total_step_limit:\n",
    "                print(\"Reached total step limit of: \" + str(total_step_limit))\n",
    "                exit(0)\n",
    "            total_step += 1\n",
    "            \n",
    "            step += 1\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            action = game_model.move(current_state)\n",
    "            next_state, reward, terminal, info = env.step(action)\n",
    "            if clip:\n",
    "                np.sign(reward)\n",
    "            score += reward\n",
    "            game_model.remember(current_state, action, reward, next_state, terminal)\n",
    "            current_state = next_state\n",
    "\n",
    "            game_model.step_update(total_step)\n",
    "\n",
    "            if terminal:\n",
    "                game_model.save_run(score, step, run)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'gym_wrappers.NoopResetEnv'> doesn't implement 'step' method, which is required for wrappers derived directly from Wrapper. Deprecated default implementation is used.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: (min: 0.0, avg: 0.6, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 0.6}\n",
      "step: (min: 166, avg: 201.7, max: 304\n",
      "{\"metric\": \"step\", \"value\": 201.7}\n",
      "{\"metric\": \"run\", \"value\": 10}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  10\n",
      "score: (min: 0.0, avg: 1.7, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.7}\n",
      "step: (min: 172, avg: 265.6, max: 400\n",
      "{\"metric\": \"step\", \"value\": 265.6}\n",
      "{\"metric\": \"run\", \"value\": 20}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  20\n",
      "score: (min: 0.0, avg: 0.6, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 0.6}\n",
      "step: (min: 162, avg: 198.3, max: 303\n",
      "{\"metric\": \"step\", \"value\": 198.3}\n",
      "{\"metric\": \"run\", \"value\": 30}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  30\n",
      "score: (min: 0.0, avg: 0.8, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 0.8}\n",
      "step: (min: 164, avg: 210.9, max: 318\n",
      "{\"metric\": \"step\", \"value\": 210.9}\n",
      "{\"metric\": \"run\", \"value\": 40}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  40\n",
      "score: (min: 0.0, avg: 1.4, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 164, avg: 246.4, max: 399\n",
      "{\"metric\": \"step\", \"value\": 246.4}\n",
      "{\"metric\": \"run\", \"value\": 50}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  50\n",
      "score: (min: 1.0, avg: 1.5, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 1.5}\n",
      "step: (min: 199, avg: 237.6, max: 429\n",
      "{\"metric\": \"step\", \"value\": 237.6}\n",
      "{\"metric\": \"run\", \"value\": 60}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  60\n",
      "score: (min: 0.0, avg: 1.6, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.6}\n",
      "step: (min: 164, avg: 266.6, max: 372\n",
      "{\"metric\": \"step\", \"value\": 266.6}\n",
      "{\"metric\": \"run\", \"value\": 70}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  70\n",
      "score: (min: 0.0, avg: 1.3, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.3}\n",
      "step: (min: 169, avg: 246.6, max: 394\n",
      "{\"metric\": \"step\", \"value\": 246.6}\n",
      "{\"metric\": \"run\", \"value\": 80}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  80\n",
      "score: (min: 0.0, avg: 1.6, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.6}\n",
      "step: (min: 166, avg: 259.2, max: 329\n",
      "{\"metric\": \"step\", \"value\": 259.2}\n",
      "{\"metric\": \"run\", \"value\": 90}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  90\n",
      "score: (min: 0.0, avg: 1.4, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 169, avg: 248, max: 379\n",
      "{\"metric\": \"step\", \"value\": 248}\n",
      "{\"metric\": \"run\", \"value\": 100}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  100\n",
      "score: (min: 0.0, avg: 1.4, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 158, avg: 246.5, max: 426\n",
      "{\"metric\": \"step\", \"value\": 246.5}\n",
      "{\"metric\": \"run\", \"value\": 110}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  110\n",
      "score: (min: 0.0, avg: 1.6, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 1.6}\n",
      "step: (min: 171, avg: 259.5, max: 302\n",
      "{\"metric\": \"step\", \"value\": 259.5}\n",
      "{\"metric\": \"run\", \"value\": 120}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  120\n",
      "score: (min: 0.0, avg: 1.1, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 167, avg: 224.7, max: 311\n",
      "{\"metric\": \"step\", \"value\": 224.7}\n",
      "{\"metric\": \"run\", \"value\": 130}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  130\n",
      "score: (min: 0.0, avg: 0.8, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 0.8}\n",
      "step: (min: 166, avg: 218.6, max: 302\n",
      "{\"metric\": \"step\", \"value\": 218.6}\n",
      "{\"metric\": \"run\", \"value\": 140}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  140\n",
      "score: (min: 0.0, avg: 1.1, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 169, avg: 236.3, max: 335\n",
      "{\"metric\": \"step\", \"value\": 236.3}\n",
      "{\"metric\": \"run\", \"value\": 150}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  150\n",
      "score: (min: 0.0, avg: 1.6, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.6}\n",
      "step: (min: 169, avg: 251.7, max: 374\n",
      "{\"metric\": \"step\", \"value\": 251.7}\n",
      "{\"metric\": \"run\", \"value\": 160}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  160\n",
      "score: (min: 0.0, avg: 1.4, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 170, avg: 249.1, max: 462\n",
      "{\"metric\": \"step\", \"value\": 249.1}\n",
      "{\"metric\": \"run\", \"value\": 170}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  170\n",
      "score: (min: 0.0, avg: 1.5, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.5}\n",
      "step: (min: 163, avg: 246.5, max: 371\n",
      "{\"metric\": \"step\", \"value\": 246.5}\n",
      "{\"metric\": \"run\", \"value\": 180}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  180\n",
      "score: (min: 0.0, avg: 1.4, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 166, avg: 253.4, max: 432\n",
      "{\"metric\": \"step\", \"value\": 253.4}\n",
      "{\"metric\": \"run\", \"value\": 190}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  190\n",
      "score: (min: 0.0, avg: 1.7, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.7}\n",
      "step: (min: 168, avg: 262.7, max: 372\n",
      "{\"metric\": \"step\", \"value\": 262.7}\n",
      "{\"metric\": \"run\", \"value\": 200}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  200\n",
      "score: (min: 0.0, avg: 1.0, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 1.0}\n",
      "step: (min: 145, avg: 219.2, max: 277\n",
      "{\"metric\": \"step\", \"value\": 219.2}\n",
      "{\"metric\": \"run\", \"value\": 210}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  210\n",
      "score: (min: 0.0, avg: 1.4, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 164, avg: 245.7, max: 339\n",
      "{\"metric\": \"step\", \"value\": 245.7}\n",
      "{\"metric\": \"run\", \"value\": 220}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  220\n",
      "loss: (min: 0.013089018873870373, avg: 0.04048342229146511, max: 5\n",
      "{\"metric\": \"loss\", \"value\": 0.04048342229146511}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 2.0658984319666773, avg: 2.0693592328959944, max: 2.0731454888032377\n",
      "{\"metric\": \"q\", \"value\": 2.0693592328959944}\n",
      "score: (min: 0.0, avg: 1.1, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 167, avg: 226.7, max: 341\n",
      "{\"metric\": \"step\", \"value\": 226.7}\n",
      "{\"metric\": \"run\", \"value\": 230}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  230\n",
      "score: (min: 0.0, avg: 0.8, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 0.8}\n",
      "step: (min: 149, avg: 211.1, max: 335\n",
      "{\"metric\": \"step\", \"value\": 211.1}\n",
      "{\"metric\": \"run\", \"value\": 240}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  240\n",
      "loss: (min: 0.010249401442706585, avg: 0.0168104897281155, max: 0.030427485704421997\n",
      "{\"metric\": \"loss\", \"value\": 0.0168104897281155}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 2.0660545097313823, avg: 2.069548143379557, max: 2.073253484531492\n",
      "{\"metric\": \"q\", \"value\": 2.069548143379557}\n",
      "score: (min: 0.0, avg: 2.1, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 2.1}\n",
      "step: (min: 178, avg: 301.3, max: 405\n",
      "{\"metric\": \"step\", \"value\": 301.3}\n",
      "{\"metric\": \"run\", \"value\": 250}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  250\n",
      "loss: (min: 0.009798349812626839, avg: 0.014242175656370818, max: 0.02652839571237564\n",
      "{\"metric\": \"loss\", \"value\": 0.014242175656370818}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 2.0646440784377607, avg: 2.0692514249780234, max: 2.0734535791267454\n",
      "{\"metric\": \"q\", \"value\": 2.0692514249780234}\n",
      "score: (min: 0.0, avg: 1.4, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: (min: 169, avg: 253, max: 335\n",
      "{\"metric\": \"step\", \"value\": 253}\n",
      "{\"metric\": \"run\", \"value\": 260}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  260\n",
      "score: (min: 0.0, avg: 1.1, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 163, avg: 227.9, max: 365\n",
      "{\"metric\": \"step\", \"value\": 227.9}\n",
      "{\"metric\": \"run\", \"value\": 270}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  270\n",
      "loss: (min: 0.00868997536599636, avg: 0.013094606350176036, max: 0.02207101322710514\n",
      "{\"metric\": \"loss\", \"value\": 0.013094606350176036}\n",
      "accuracy: (min: 0, avg: 0, max: 0\n",
      "{\"metric\": \"accuracy\", \"value\": 0}\n",
      "q: (min: 2.0646990274530648, avg: 2.0684605926890614, max: 2.071810661956966\n",
      "{\"metric\": \"q\", \"value\": 2.0684605926890614}\n",
      "score: (min: 0.0, avg: 1.8, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.8}\n",
      "step: (min: 171, avg: 268.1, max: 353\n",
      "{\"metric\": \"step\", \"value\": 268.1}\n",
      "{\"metric\": \"run\", \"value\": 280}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RUN =  280\n"
     ]
    }
   ],
   "source": [
    "game_model = DDQNTrainer(game_name, INPUT_SHAPE, env.action_space.n)\n",
    "main_loop(game_model, env, render, total_step_limit, total_run_limit, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-05-05'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(datetime.datetime.now().strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_FREQUENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNTrainer:\n",
    "    def __init__(self, game_name, input_shape, action_space):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.game_name = game_name\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.game_mode = \"DDQN training\"\n",
    "        self.model_path = \"./output/neural_nets/\" + game_name + \"/ddqn/\" + self._get_date() + \"/model.h5\"\n",
    "        self.logger_path = \"./output/logs/\" + game_name + \"/ddqn/training/\" + self._get_date() + \"/\"\n",
    "        self.logger = Logger(self.game_name + \" \" + self.game_mode, self.logger_path)\n",
    "        \n",
    "        if os.path.exists(os.path.dirname(self.model_path)):\n",
    "            shutil.rmtree(os.path.dirname(self.model_path), ignore_errors=True)\n",
    "        os.makedirs(os.path.dirname(self.model_path))\n",
    "\n",
    "        self.ddqn = ConvolutionalNeuralNetwork(self.input_shape, action_space).to(device)\n",
    "        \n",
    "        self.criteria = nn.MSELoss()\n",
    "        self.optimizer = RMSprop(self.ddqn.parameters(), lr=0.00025, alpha = 0.95, eps=0.01)\n",
    "        \n",
    "        if os.path.isfile(self.model_path):\n",
    "            self.ddqn.load_state_dict(torch.load(self.model_path))\n",
    "        \n",
    "        self.ddqn_target = ConvolutionalNeuralNetwork(self.input_shape, action_space).to(device)\n",
    "#         .model\n",
    "        self._reset_target_network()\n",
    "        self.epsilon = EXPLORATION_MAX\n",
    "        self.memory = []\n",
    "    \n",
    "    def _save_model(self):\n",
    "        torch.save(self.ddqn.state_dict(),self.model_path)\n",
    "#         self.ddqn.save_weights(self.model_path)\n",
    "        \n",
    "    def _get_date(self):\n",
    "        return str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "        \n",
    "    def save_run(self, score, step, run):\n",
    "        self.logger.add_score(score)\n",
    "        self.logger.add_step(step)\n",
    "        self.logger.add_run(run)\n",
    "\n",
    "    def move(self, state):\n",
    "        if np.random.rand() < self.epsilon or len(self.memory) < REPLAY_START_SIZE:\n",
    "            return random.randrange(self.action_space)\n",
    "        \n",
    "        q_values = self.ddqn(np.expand_dims(np.asarray(state).astype(np.float64), axis=0), batch_size=1)\n",
    "        return np.argmax(q_values[0].detach().cpu().numpy())\n",
    "\n",
    "    def remember(self, current_state, action, reward, next_state, terminal):\n",
    "        self.memory.append({\"current_state\": current_state,\n",
    "                            \"action\": action,\n",
    "                            \"reward\": reward,\n",
    "                            \"next_state\": next_state,\n",
    "                            \"terminal\": terminal})\n",
    "        if len(self.memory) > MEMORY_SIZE:\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def step_update(self, total_step):\n",
    "        if len(self.memory) < REPLAY_START_SIZE:\n",
    "            return\n",
    "\n",
    "        if total_step % TRAINING_FREQUENCY == 0:\n",
    "            loss, accuracy, average_max_q = self._train()\n",
    "            self.logger.add_loss(loss.item())\n",
    "            self.logger.add_accuracy(accuracy)\n",
    "            self.logger.add_q(average_max_q)\n",
    "\n",
    "        self._update_epsilon()\n",
    "\n",
    "        if total_step % MODEL_PERSISTENCE_UPDATE_FREQUENCY == 0:\n",
    "            self._save_model()\n",
    "\n",
    "        if total_step % TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "            self._reset_target_network()\n",
    "            print('{{\"metric\": \"epsilon\", \"value\": {}}}'.format(self.epsilon))\n",
    "            print('{{\"metric\": \"total_step\", \"value\": {}}}'.format(total_step))\n",
    "\n",
    "    def _train(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        batch = np.asarray(random.sample(self.memory, BATCH_SIZE))\n",
    "        if len(batch) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        current_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        terminals = []\n",
    "        \n",
    "        q_values = []\n",
    "        max_q_values = []\n",
    "        \n",
    "#         print(batch)\n",
    "        for entry in batch:\n",
    "            current_state = np.expand_dims(np.asarray(entry[\"current_state\"]).astype(np.float64), axis=0)\n",
    "            current_states.append(current_state)\n",
    "            next_state = np.expand_dims(np.asarray(entry[\"next_state\"]).astype(np.float64), axis=0)\n",
    "            \n",
    "            next_states.append(next_state)\n",
    "            rewards.append(entry['reward'])\n",
    "            actions.append(entry['action'])\n",
    "            terminals.append(entry['terminal'])\n",
    "            \n",
    "        non_final_mask = torch.tensor(terminals)==False\n",
    "        non_final_next_states = np.asarray(next_states).squeeze()\n",
    "        \n",
    "        state_batch = np.asarray(current_states).squeeze()\n",
    "        action_batch = np.asanyarray(actions).squeeze()\n",
    "        reward_batch = np.asarray(rewards).squeeze()\n",
    "#         print(torch.from_numpy(action_batch).unsqueeze(dim=1).size(),torch.Size([32, 1]))\n",
    "        state_action_values = self.ddqn(state_batch).gather(1, torch.from_numpy(action_batch).unsqueeze(dim=1).to(device))\n",
    "    \n",
    "#         next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        next_state_values = np.zeros(BATCH_SIZE)\n",
    "        next_state_values[non_final_mask] = np.max(self.ddqn_target(non_final_next_states).detach().cpu().numpy())\n",
    "#         .max(1)[0].detach().cpu()    \n",
    "    \n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "        \n",
    "        loss = self.criteria(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "#         print(state_action_values.size(),[32, 4])\n",
    "#         print(state_action_values.size(),torch.Size([32, 1]))\n",
    "#         print(non_final_next_states.squeeze().size(), torch.Size([32, 4, 84, 84]))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "#             next_state_prediction = self.ddqn_target(next_state).detach().cpu().numpy().ravel()\n",
    "# #             print(next_state_prediction.shape, (4,))\n",
    "            \n",
    "#             next_q_value = np.max(next_state_prediction)\n",
    "#             q = list(self.ddqn(current_state)[0].detach().cpu().numpy())\n",
    "# #             print(len(q), 4)\n",
    "#             if entry[\"terminal\"]:\n",
    "#                 q[entry[\"action\"]] = entry[\"reward\"]\n",
    "#             else:\n",
    "#                 q[entry[\"action\"]] = entry[\"reward\"] + GAMMA * next_q_value\n",
    "#             q_values.append(q)\n",
    "#             max_q_values.append(np.max(q))\n",
    "        \n",
    "#         model_out = self.ddqn(np.asarray(current_states).squeeze())\n",
    "#         loss = self.criteria(model_out, torch.from_numpy(np.array(q_values)).float().to(device))\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "    \n",
    "\n",
    "        accuracy = 0\n",
    "        return loss, accuracy, mean(max_q_values)\n",
    "\n",
    "    def _update_epsilon(self):\n",
    "        self.epsilon -= EXPLORATION_DECAY\n",
    "        self.epsilon = max(EXPLORATION_MIN, self.epsilon)\n",
    "\n",
    "    def _reset_target_network(self):\n",
    "        self.ddqn_target.load_state_dict(self.ddqn.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
