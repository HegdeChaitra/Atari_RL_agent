{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import itertools\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import argparse\n",
    "from torch import optim\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "from torch.utils.data import Sampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import seaborn\n",
    "import datetime\n",
    "from logger import Logger\n",
    "\n",
    "from torch.optim import RMSprop\n",
    "import shutil\n",
    "from statistics import mean\n",
    "\n",
    "from gym_wrappers import MainGymWrapper\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, action_space):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.action_space = action_space\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=8, stride=(4,4))\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64,kernel_size=4,stride=(2,2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        o_size = 3136\n",
    "        self.linear1 = nn.Linear(in_features=o_size, out_features=512)\n",
    "        self.linear2 = nn.Linear(in_features=512, out_features=self.action_space)\n",
    "        \n",
    "    def forward(self, x, batch_size=None):\n",
    "        x = torch.from_numpy(x).float()\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "\n",
    "        # out = (bs, nc, x, y)\n",
    "        if batch_size==None:\n",
    "            batch_size = out.size(0)\n",
    "#         print(out.size())\n",
    "        out_flat = out.view(batch_size, -1)\n",
    "        \n",
    "        out_flat = F.relu(self.linear1(out_flat))\n",
    "        \n",
    "        out_flat = self.linear2(out_flat)\n",
    "        return out_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = \"Breakout-v0\"\n",
    "game_mode = 'ddqn_training'\n",
    "render = False\n",
    "total_step_limit = 5000000\n",
    "total_run_limit = None\n",
    "clip = True\n",
    "\n",
    "FRAMES_IN_OBSERVATION = 4\n",
    "FRAME_SIZE = 84\n",
    "INPUT_SHAPE = (FRAMES_IN_OBSERVATION, FRAME_SIZE, FRAME_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Breakout-v0', 'Breakout-v0Deterministic-v4')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = game_name + \"Deterministic-v4\" \n",
    "game_name, env_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = MainGymWrapper.wrap(gym.make(game_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "MEMORY_SIZE = 900000\n",
    "BATCH_SIZE = 32\n",
    "TRAINING_FREQUENCY = 4\n",
    "TARGET_NETWORK_UPDATE_FREQUENCY = 40000\n",
    "MODEL_PERSISTENCE_UPDATE_FREQUENCY = 10000\n",
    "REPLAY_START_SIZE = 50000\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.1\n",
    "EXPLORATION_TEST = 0.02\n",
    "EXPLORATION_STEPS = 850000\n",
    "EXPLORATION_DECAY = (EXPLORATION_MAX-EXPLORATION_MIN)/EXPLORATION_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNTrainer:\n",
    "    def __init__(self, game_name, input_shape, action_space):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.game_name = game_name\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.game_mode = \"DDQN training\"\n",
    "        self.model_path = \"./output/neural_nets/\" + game_name + \"/ddqn/\" + self._get_date() + \"/model.h5\"\n",
    "        self.logger_path = \"./output/logs/\" + game_name + \"/ddqn/training/\" + self._get_date() + \"/\"\n",
    "        self.logger = Logger(self.game_name + \" \" + self.game_mode, self.logger_path)\n",
    "        \n",
    "        if os.path.exists(os.path.dirname(self.model_path)):\n",
    "            shutil.rmtree(os.path.dirname(self.model_path), ignore_errors=True)\n",
    "        os.makedirs(os.path.dirname(self.model_path))\n",
    "\n",
    "        self.ddqn = ConvolutionalNeuralNetwork(self.input_shape, action_space)\n",
    "        \n",
    "        self.criteria = nn.CrossEntropyLoss()\n",
    "        self.optimizer = RMSprop(self.ddqn.parameters(), lr=0.00025, alpha = 0.95, eps=0.01)\n",
    "        \n",
    "        if os.path.isfile(self.model_path):\n",
    "            self.ddqn.load_state_dict(self.model_path)\n",
    "        \n",
    "        self.ddqn_target = ConvolutionalNeuralNetwork(self.input_shape, action_space)\n",
    "#         .model\n",
    "        self._reset_target_network()\n",
    "        self.epsilon = EXPLORATION_MAX\n",
    "        self.memory = []\n",
    "    \n",
    "    def _save_model(self):\n",
    "        torch.save(self.ddqn.state_dict(),self.model_path)\n",
    "#         self.ddqn.save_weights(self.model_path)\n",
    "        \n",
    "    def _get_date(self):\n",
    "        return str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "        \n",
    "    def save_run(self, score, step, run):\n",
    "        self.logger.add_score(score)\n",
    "        self.logger.add_step(step)\n",
    "        self.logger.add_run(run)\n",
    "\n",
    "    def move(self, state):\n",
    "        if np.random.rand() < self.epsilon or len(self.memory) < REPLAY_START_SIZE:\n",
    "            return random.randrange(self.action_space)\n",
    "        \n",
    "        q_values = self.ddqn(np.expand_dims(np.asarray(state).astype(np.float64), axis=0), batch_size=1)\n",
    "        return np.argmax(q_values[0].detach().numpy())\n",
    "\n",
    "    def remember(self, current_state, action, reward, next_state, terminal):\n",
    "        self.memory.append({\"current_state\": current_state,\n",
    "                            \"action\": action,\n",
    "                            \"reward\": reward,\n",
    "                            \"next_state\": next_state,\n",
    "                            \"terminal\": terminal})\n",
    "        if len(self.memory) > MEMORY_SIZE:\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def step_update(self, total_step):\n",
    "        if len(self.memory) < REPLAY_START_SIZE:\n",
    "            return\n",
    "\n",
    "        if total_step % TRAINING_FREQUENCY == 0:\n",
    "            loss, accuracy, average_max_q = self._train()\n",
    "            self.logger.add_loss(loss.item())\n",
    "            self.logger.add_accuracy(accuracy)\n",
    "            self.logger.add_q(average_max_q)\n",
    "\n",
    "        self._update_epsilon()\n",
    "\n",
    "        if total_step % MODEL_PERSISTENCE_UPDATE_FREQUENCY == 0:\n",
    "            self._save_model()\n",
    "\n",
    "        if total_step % TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "            self._reset_target_network()\n",
    "            print('{{\"metric\": \"epsilon\", \"value\": {}}}'.format(self.epsilon))\n",
    "            print('{{\"metric\": \"total_step\", \"value\": {}}}'.format(total_step))\n",
    "\n",
    "    def _train(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        batch = np.asarray(random.sample(self.memory, BATCH_SIZE))\n",
    "        if len(batch) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        current_states = []\n",
    "        q_values = []\n",
    "        max_q_values = []\n",
    "\n",
    "        for entry in batch:\n",
    "            current_state = np.expand_dims(np.asarray(entry[\"current_state\"]).astype(np.float64), axis=0)\n",
    "            current_states.append(current_state)\n",
    "            next_state = np.expand_dims(np.asarray(entry[\"next_state\"]).astype(np.float64), axis=0)\n",
    "            \n",
    "            next_state_prediction = self.ddqn_target(next_state).detach().numpy().ravel()\n",
    "            \n",
    "            next_q_value = np.max(next_state_prediction)\n",
    "            q = list(self.ddqn(current_state)[0].detach().numpy())\n",
    "            \n",
    "            if entry[\"terminal\"]:\n",
    "                q[entry[\"action\"]] = entry[\"reward\"]\n",
    "            else:\n",
    "                q[entry[\"action\"]] = entry[\"reward\"] + GAMMA * next_q_value\n",
    "            q_values.append(q)\n",
    "            max_q_values.append(np.max(q))\n",
    "        \n",
    "#         print(np.array(q_values).shape)\n",
    "        #TODO : Investigate this whole two model code \n",
    "        model_out = self.ddqn(np.asarray(current_states).squeeze())\n",
    "#         print(model_out.size())\n",
    "        \n",
    "#         fit = self.ddqn.fit(np.asarray(current_states).squeeze(),\n",
    "#                             np.asarray(q_values).squeeze(),\n",
    "#                             batch_size=BATCH_SIZE,\n",
    "#                             verbose=0)\n",
    "\n",
    "        #TODO : Check proper input to loss function\n",
    "        loss = self.criteria(model_out, torch.max(torch.from_numpy(np.array(q_values)).long(),1)[1])\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "\n",
    "#         loss = fit.history[\"loss\"][0]\n",
    "#         accuracy = fit.history[\"acc\"][0]\n",
    "        #TODO: Code accuracy\n",
    "        accuracy = 0\n",
    "        return loss, accuracy, mean(max_q_values)\n",
    "\n",
    "    def _update_epsilon(self):\n",
    "        self.epsilon -= EXPLORATION_DECAY\n",
    "        self.epsilon = max(EXPLORATION_MIN, self.epsilon)\n",
    "\n",
    "    def _reset_target_network(self):\n",
    "        self.ddqn_target.load_state_dict(self.ddqn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(game_model, env, render, total_step_limit, total_run_limit, clip):\n",
    "    run = 0\n",
    "    total_step = 0\n",
    "    while True:\n",
    "        if total_run_limit is not None and run >= total_run_limit:\n",
    "            print(\"Reached total run limit of: \" + str(total_run_limit))\n",
    "            exit(0)\n",
    "\n",
    "        run += 1\n",
    "        current_state = env.reset()\n",
    "        step = 0\n",
    "        score = 0\n",
    "        while True:\n",
    "            if total_step >= total_step_limit:\n",
    "                print(\"Reached total step limit of: \" + str(total_step_limit))\n",
    "                exit(0)\n",
    "            total_step += 1\n",
    "            step += 1\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            action = game_model.move(current_state)\n",
    "            next_state, reward, terminal, info = env.step(action)\n",
    "            if clip:\n",
    "                np.sign(reward)\n",
    "            score += reward\n",
    "            game_model.remember(current_state, action, reward, next_state, terminal)\n",
    "            current_state = next_state\n",
    "\n",
    "            game_model.step_update(total_step)\n",
    "\n",
    "            if terminal:\n",
    "                game_model.save_run(score, step, run)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: (min: 0.0, avg: 1.5, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.5}\n",
      "step: (min: 173, avg: 251.1, max: 393\n",
      "{\"metric\": \"step\", \"value\": 251.1}\n",
      "{\"metric\": \"run\", \"value\": 10}\n",
      "score: (min: 0.0, avg: 1.1, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 170, avg: 228.7, max: 377\n",
      "{\"metric\": \"step\", \"value\": 228.7}\n",
      "{\"metric\": \"run\", \"value\": 20}\n",
      "score: (min: 0.0, avg: 1.7, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 1.7}\n",
      "step: (min: 160, avg: 266.6, max: 472\n",
      "{\"metric\": \"step\", \"value\": 266.6}\n",
      "{\"metric\": \"run\", \"value\": 30}\n",
      "score: (min: 0.0, avg: 0.5, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 0.5}\n",
      "step: (min: 155, avg: 193.8, max: 336\n",
      "{\"metric\": \"step\", \"value\": 193.8}\n",
      "{\"metric\": \"run\", \"value\": 40}\n",
      "score: (min: 0.0, avg: 1.0, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.0}\n",
      "step: (min: 171, avg: 229.6, max: 378\n",
      "{\"metric\": \"step\", \"value\": 229.6}\n",
      "{\"metric\": \"run\", \"value\": 50}\n",
      "score: (min: 0.0, avg: 1.2, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 1.2}\n",
      "step: (min: 164, avg: 232.4, max: 392\n",
      "{\"metric\": \"step\", \"value\": 232.4}\n",
      "{\"metric\": \"run\", \"value\": 60}\n",
      "score: (min: 0.0, avg: 0.9, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 0.9}\n",
      "step: (min: 153, avg: 222.5, max: 314\n",
      "{\"metric\": \"step\", \"value\": 222.5}\n",
      "{\"metric\": \"run\", \"value\": 70}\n",
      "score: (min: 0.0, avg: 1.7, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 1.7}\n",
      "step: (min: 174, avg: 253.6, max: 461\n",
      "{\"metric\": \"step\", \"value\": 253.6}\n",
      "{\"metric\": \"run\", \"value\": 80}\n",
      "score: (min: 0.0, avg: 1.6, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 1.6}\n",
      "step: (min: 167, avg: 252.2, max: 407\n",
      "{\"metric\": \"step\", \"value\": 252.2}\n",
      "{\"metric\": \"run\", \"value\": 90}\n",
      "score: (min: 0.0, avg: 2.2, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 2.2}\n",
      "step: (min: 157, avg: 290.7, max: 391\n",
      "{\"metric\": \"step\", \"value\": 290.7}\n",
      "{\"metric\": \"run\", \"value\": 100}\n",
      "score: (min: 0.0, avg: 1.6, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.6}\n",
      "step: (min: 168, avg: 255.4, max: 372\n",
      "{\"metric\": \"step\", \"value\": 255.4}\n",
      "{\"metric\": \"run\", \"value\": 110}\n",
      "score: (min: 0.0, avg: 1.4, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 163, avg: 250, max: 363\n",
      "{\"metric\": \"step\", \"value\": 250}\n",
      "{\"metric\": \"run\", \"value\": 120}\n",
      "score: (min: 0.0, avg: 2.0, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 2.0}\n",
      "step: (min: 157, avg: 273.8, max: 431\n",
      "{\"metric\": \"step\", \"value\": 273.8}\n",
      "{\"metric\": \"run\", \"value\": 130}\n",
      "score: (min: 0.0, avg: 1.5, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.5}\n",
      "step: (min: 172, avg: 256.1, max: 395\n",
      "{\"metric\": \"step\", \"value\": 256.1}\n",
      "{\"metric\": \"run\", \"value\": 140}\n",
      "score: (min: 0.0, avg: 1.4, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.4}\n",
      "step: (min: 158, avg: 244.8, max: 344\n",
      "{\"metric\": \"step\", \"value\": 244.8}\n",
      "{\"metric\": \"run\", \"value\": 150}\n",
      "score: (min: 0.0, avg: 1.8, max: 5.0\n",
      "{\"metric\": \"score\", \"value\": 1.8}\n",
      "step: (min: 177, avg: 275.4, max: 438\n",
      "{\"metric\": \"step\", \"value\": 275.4}\n",
      "{\"metric\": \"run\", \"value\": 160}\n",
      "score: (min: 0.0, avg: 0.6, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 0.6}\n",
      "step: (min: 158, avg: 198.9, max: 344\n",
      "{\"metric\": \"step\", \"value\": 198.9}\n",
      "{\"metric\": \"run\", \"value\": 170}\n",
      "score: (min: 0.0, avg: 1.1, max: 3.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 168, avg: 237.9, max: 375\n",
      "{\"metric\": \"step\", \"value\": 237.9}\n",
      "{\"metric\": \"run\", \"value\": 180}\n",
      "score: (min: 0.0, avg: 1.0, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 1.0}\n",
      "step: (min: 164, avg: 231.5, max: 289\n",
      "{\"metric\": \"step\", \"value\": 231.5}\n",
      "{\"metric\": \"run\", \"value\": 190}\n",
      "score: (min: 0.0, avg: 1.5, max: 4.0\n",
      "{\"metric\": \"score\", \"value\": 1.5}\n",
      "step: (min: 160, avg: 247.6, max: 403\n",
      "{\"metric\": \"step\", \"value\": 247.6}\n",
      "{\"metric\": \"run\", \"value\": 200}\n",
      "score: (min: 0.0, avg: 1.1, max: 2.0\n",
      "{\"metric\": \"score\", \"value\": 1.1}\n",
      "step: (min: 163, avg: 228.9, max: 299\n",
      "{\"metric\": \"step\", \"value\": 228.9}\n",
      "{\"metric\": \"run\", \"value\": 210}\n",
      "score: (min: 0.0, avg: 1.7, max: 8.0\n",
      "{\"metric\": \"score\", \"value\": 1.7}\n",
      "step: (min: 144, avg: 252.9, max: 496\n",
      "{\"metric\": \"step\", \"value\": 252.9}\n",
      "{\"metric\": \"run\", \"value\": 220}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert type 'Tensor' to numerator/denominator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-46f1145dd9a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgame_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDQNTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_SHAPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_step_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_run_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-162-a5b3228ea66a>\u001b[0m in \u001b[0;36mmain_loop\u001b[0;34m(game_model, env, render, total_step_limit, total_run_limit, clip)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mgame_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-161-5c8e8a9e7ff0>\u001b[0m in \u001b[0;36mstep_update\u001b[0;34m(self, total_step)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mTRAINING_FREQUENCY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_max_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_max_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/atari_game/logger.py\u001b[0m in \u001b[0;36madd_loss\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_LOSS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Loss clipping for very big values that are likely to happen in the early stages of learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/atari_game/logger.py\u001b[0m in \u001b[0;36madd_entry\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mmean_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_label\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\": (min: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", avg: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", max: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{\"metric\": \"'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_label\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\", \"value\": {}}}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/python3/3.6.3/intel/lib/python3.6/statistics.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mStatisticsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean requires at least one data point'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/python3/3.6.3/intel/lib/python3.6/statistics.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(data, start)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_coerce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# or raise TypeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_exact_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mpartials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartials_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/python3/3.6.3/intel/lib/python3.6/statistics.py\u001b[0m in \u001b[0;36m_exact_ratio\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"can't convert type '{}' to numerator/denominator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert type 'Tensor' to numerator/denominator"
     ]
    }
   ],
   "source": [
    "game_model = DDQNTrainer(game_mode, INPUT_SHAPE, env.action_space.n)\n",
    "main_loop(game_model, env, render, total_step_limit, total_run_limit, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
